{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 静态图(TensorFlow 1.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OP:\n",
    "    def __init__(self):\n",
    "        self.name = self.__class__.__name__\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        self.input = args\n",
    "        self.output = Placeholder()\n",
    "        self.output.op = self\n",
    "        return self.output\n",
    "\n",
    "    def compute(self):\n",
    "\n",
    "        new_input = []\n",
    "        for index, item in enumerate(self.input):  # input在推理的时候记录下来\n",
    "            if isinstance(item, Tensor):\n",
    "                if item.op is not None:\n",
    "                    item = item.op.compute()  # 将placeholder转变为确定的值\n",
    "            new_input.append(item)\n",
    "\n",
    "        self.input = new_input\n",
    "        output = self.forward(*new_input)\n",
    "        output.op = self\n",
    "        return output\n",
    "\n",
    "    def forward(self, *args):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, *args):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_native(self, grad):\n",
    "        # 可以把backward理解为求导\n",
    "        input_grads = self.backward(grad)\n",
    "        if not isinstance(input_grads, tuple):\n",
    "            input_grads = (input_grads, )\n",
    "\n",
    "        assert len(input_grads) == len(self.input), \"Number grads mismatch number input\"\n",
    "\n",
    "        for ig, ip in zip(input_grads, self.input):\n",
    "            if isinstance(ip, Tensor):\n",
    "                ip.backward(ig)\n",
    "\n",
    "    def get_data(self, item):\n",
    "        if isinstance(item, Tensor):\n",
    "            return item.data\n",
    "        else:\n",
    "            return item\n",
    "\n",
    "\n",
    "class AddOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return Tensor(self.get_data(a) + self.get_data(b))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        '''\n",
    "            分别返回对(a+b)对a和b求导的值\n",
    "            d(a+b)/da = 1\n",
    "            d(a+b)/db = 1\n",
    "            所以(1*grad, 1*grad)\n",
    "        '''\n",
    "        return grad, grad\n",
    "\n",
    "\n",
    "class SubOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return Tensor(self.get_data(a) - self.get_data(b))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad, -1 * grad\n",
    "\n",
    "\n",
    "class MulOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return Tensor(self.get_data(a) * self.get_data(b))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        a, b = self.input\n",
    "        return grad * self.get_data(b), grad * self.get_data(a)\n",
    "\n",
    "\n",
    "class DivOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return Tensor(self.get_data(a) / self.get_data(b))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        a, b = self.input\n",
    "        return grad / self.get_data(b), grad * self.get_data(a) / (self.get_data(b) ** 2) * -1\n",
    "\n",
    "\n",
    "class ExpOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a):\n",
    "        return Tensor(np.exp(self.get_data(a)))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad * np.exp(self.get_data(self.input[0]))\n",
    "\n",
    "\n",
    "class LogOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a):\n",
    "        return Tensor(np.log(self.get_data(a)))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad / self.get_data(self.input[0])\n",
    "\n",
    "\n",
    "class MatMulOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return Tensor(self.get_data(a) @ self.get_data(b))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        a, b = self.input\n",
    "        return grad @ self.get_data(b).T, self.get_data(a).T @ grad\n",
    "\n",
    "\n",
    "class SumOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a):\n",
    "        return Tensor(np.sum(self.get_data(a)))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        a = self.input[0]\n",
    "        return np.full_like(self.get_data(a), grad)\n",
    "\n",
    "\n",
    "class MeanOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a):\n",
    "        return Tensor(np.mean(self.get_data(a)))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        a = self.input[0]\n",
    "        d = self.get_data(a)\n",
    "        return np.full_like(d, grad / d.size)\n",
    "\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data, op=None):\n",
    "        self.data = data\n",
    "        self.op = op\n",
    "        self.grad = 0\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return AddOP()(other, self)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return AddOP()(self, other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return SubOP()(other, self)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return SubOP()(self, other)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return MulOP()(other, self)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return MulOP()(self, other)\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return DivOP()(other, self)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return DivOP()(self, other)\n",
    "\n",
    "    def __neg__(self):\n",
    "        return MulOP()(self, -1)\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        return MatMulOP()(self, other)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.op is not None:\n",
    "            return f\"tensor({self.data}, grad_fn=<{self.op.name}>)\"\n",
    "        else:\n",
    "            return f\"{self.data}\"\n",
    "\n",
    "    def backward(self, grad=1):\n",
    "        self.grad = self.grad + grad  # 就是1\n",
    "        if self.op is not None:\n",
    "            self.op.backward_native(grad)\n",
    "\n",
    "\n",
    "class Placeholder(Tensor):\n",
    "    def __init__(self):\n",
    "        super().__init__(0)\n",
    "\n",
    "\n",
    "def SessionRun(var, feed_dict):\n",
    "    for key in feed_dict:\n",
    "        key.data = feed_dict[key]\n",
    "\n",
    "    return var.op.compute()\n",
    "\n",
    "\n",
    "# 模拟包的形式\n",
    "class morch:\n",
    "\n",
    "    @staticmethod\n",
    "    def exp(value):\n",
    "        return ExpOP()(value)\n",
    "\n",
    "    @staticmethod\n",
    "    def log(value):\n",
    "        return LogOP()(value)\n",
    "\n",
    "    @staticmethod\n",
    "    def sum(value):\n",
    "        return SumOP()(value)\n",
    "\n",
    "    @staticmethod\n",
    "    def mean(value):\n",
    "        return MeanOP()(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================基于PyTorch的自动微分-动态图版本：==========================================\n",
      "计算结果是： tensor(12.53067, grad_fn=<SumBackward0>)\n",
      "a的导数是： [[0.09375 0.29492 0.27561]\n",
      " [0.01694 0.02649 0.01745]\n",
      " [0.00092 0.00137 0.00088]]\n",
      "b的导数是： [[2.4501  2.4501  2.4501 ]\n",
      " [2.71216 2.71216 2.71216]\n",
      " [2.87377 2.87377 2.87377]]\n",
      "\n",
      "\n",
      "\n",
      "==========================================自己写的自动微分-静态图版本：==========================================\n",
      "计算结果是： tensor(12.530673399567604, grad_fn=<SumOP>)\n",
      "dt/da： [[0.09375 0.29492 0.27561]\n",
      " [0.01694 0.02649 0.01745]\n",
      " [0.00092 0.00137 0.00088]]\n",
      "dt/db： [[2.4501  2.4501  2.4501 ]\n",
      " [2.71216 2.71216 2.71216]\n",
      " [2.87377 2.87377 2.87377]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class OP:\n",
    "    def __init__(self):\n",
    "        self.name = self.__class__.__name__\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        '''\n",
    "            每一次eval表达式，不但把结果输出\n",
    "            还把表达式本身放在结果里\n",
    "\n",
    "            这可能是静态图的作用，因为变量的值是从顶层传入的（最后时刻传入）\n",
    "\n",
    "            所以一个复杂表达式后:\n",
    "            a = sum(1/e^(-x) @ M)\n",
    "            先后__call__了这几个\n",
    "                __neg__, __exp__, __truediv__, __matmul__, __sum__\n",
    "            此时a.op就是__sum__这个operator, input(即参数)就是前面一层的__matmul__的output\n",
    "            同理，__matmul__的input是__truediv__，\n",
    "            这个遍历是在compute函数里进行的，所以只要在顶层调用compute，将会用input（即args）来\n",
    "            依次往下遍历完。\n",
    "\n",
    "            output.op是往上传的，所以通过每个入参的op能找到它的下一级\n",
    "        '''\n",
    "        self.input = args\n",
    "        self.output = Placeholder()\n",
    "        self.output.op = self\n",
    "        return self.output\n",
    "\n",
    "    def compute(self):\n",
    "\n",
    "        new_input = []\n",
    "        for index, item in enumerate(self.input):  # input在推理的时候记录下来\n",
    "            if isinstance(item, Tensor):\n",
    "                # item.op等于成了是否是计算结果还是单纯一个表达式的标识\n",
    "                # 如果是放结果，op必然指向计算它的表达式\n",
    "                if item.op is not None:\n",
    "                    # 这里递归调到最底层的-a（此时a已经有值）, 依次把output层层计算上来\n",
    "                    item = item.op.compute()  # 将placeholder转变为确定的值\n",
    "            new_input.append(item)\n",
    "\n",
    "        self.input = new_input\n",
    "        output = self.forward(*new_input)\n",
    "        output.op = self\n",
    "        return output\n",
    "\n",
    "    def forward(self, *args):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, *args):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_native(self, grad):\n",
    "        # 可以把backward理解为求导\n",
    "        input_grads = self.backward(grad)\n",
    "        # 在例子中，标量结果来自于矩阵求和，这个方法就处于矩阵求和的方法体内\n",
    "        # 找到矩阵求和的求导方法，就把导数包成了全1矩阵\n",
    "        if not isinstance(input_grads, tuple):\n",
    "            input_grads = (input_grads, )\n",
    "\n",
    "        assert len(input_grads) == len(self.input), \"Number grads mismatch number input\"\n",
    "\n",
    "        # 依然用这个例子，走到这一步，显然，只有一个入参，即矩阵\n",
    "        # 再把这个全1的新梯度（即导数）往这个矩阵的底层回传\n",
    "        # 因为矩阵来自于两个矩阵相乘 (a(1/e^-a) @ b)，\n",
    "        # 可见，必然找到了__matmul__的函数体中去了 （错！）\n",
    "\n",
    "        # 要知道只有Tensor才会调backward_native方法，而在这个方法里才会往参数里传入梯度\n",
    "        # 以回传，所以如果这里就跑到operator里面去了，那肯定是不能BP下去了\n",
    "\n",
    "        # 所以要知道，sum的矩阵，这个矩阵来自两个矩阵相乘没错\n",
    "        # 但首先，它是一个矩阵，即，一个Tensor\n",
    "        # 所以下面的代码在sum矩阵这一段其实是这个意思：\n",
    "        # 入参是一个矩阵，检查这个矩阵是否是一个Tensor对象\n",
    "        # 如果是，就进入该对象的backward方法\n",
    "        # 順理成章，它就进入了该tensor产生来源(op)的native方法中\n",
    "        # 这才回到这里，已经成了两个入参（矩阵）的一个乘法表达式了\n",
    "        # 这时会返回两个梯度，分别是对左取导，对右取导的\n",
    "        # 再判断左右参是否Tensor，继续回传\n",
    "\n",
    "        # 所以整个BP是通过Tensor对象回传的，OP只是一个简单的操作符，\n",
    "        # 但是却在这个操作符里通过参数，把梯度传下去\n",
    "        for ig, ip in zip(input_grads, self.input):\n",
    "            if isinstance(ip, Tensor):\n",
    "                ip.backward(ig)\n",
    "\n",
    "    def get_data(self, item):\n",
    "        if isinstance(item, Tensor):\n",
    "            return item.data\n",
    "        else:\n",
    "            return item\n",
    "\n",
    "\n",
    "class AddOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return Tensor(self.get_data(a) + self.get_data(b))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        '''\n",
    "            分别返回对(a+b)对a和b求导的值\n",
    "            d(a+b)/da = 1\n",
    "            d(a+b)/db = 1\n",
    "            所以(1*grad, 1*grad)\n",
    "        '''\n",
    "        return grad, grad\n",
    "\n",
    "\n",
    "class SubOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return Tensor(self.get_data(a) - self.get_data(b))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad, -1 * grad\n",
    "\n",
    "\n",
    "class MulOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return Tensor(self.get_data(a) * self.get_data(b))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        a, b = self.input\n",
    "        return grad * self.get_data(b), grad * self.get_data(a)\n",
    "\n",
    "\n",
    "class DivOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return Tensor(self.get_data(a) / self.get_data(b))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        a, b = self.input\n",
    "        return grad / self.get_data(b), grad * self.get_data(a) / (self.get_data(b) ** 2) * -1\n",
    "\n",
    "\n",
    "class ExpOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a):\n",
    "        return Tensor(np.exp(self.get_data(a)))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad * np.exp(self.get_data(self.input[0]))\n",
    "\n",
    "\n",
    "class LogOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a):\n",
    "        return Tensor(np.log(self.get_data(a)))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad / self.get_data(self.input[0])\n",
    "\n",
    "\n",
    "class MatMulOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return Tensor(self.get_data(a) @ self.get_data(b))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        a, b = self.input\n",
    "        return grad @ self.get_data(b).T, self.get_data(a).T @ grad\n",
    "\n",
    "\n",
    "class SumOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a):\n",
    "        return Tensor(np.sum(self.get_data(a)))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        '''\n",
    "        在例子中，最顶层是对matrix求sum，得到一个标量:12.5306...\n",
    "        所以对t = (....) 整个表达式求backward，就是对标量求导，当然为1\n",
    "        但是这个标量是由sum得到的（从op可以取出），于是就调了op的backward_native\n",
    "\n",
    "        backward_native做的第一件事就是先执行一遍自己的backward，\n",
    "        所以就进了这个sum的backward的方法\n",
    "\n",
    "        注意，只有Tensor对象才会去调这个native方法，op对象只会把计算出自身的表达式\n",
    "        的参数，分别传入当然梯度，往下传播\n",
    "        '''\n",
    "        a = self.input[0]\n",
    "        '''\n",
    "        对矩阵的每一项求导，那么那一项就是1，其它项就成了常数，变成了0\n",
    "        sum起来，就成了每一项都是1\n",
    "        '''\n",
    "        return np.full_like(self.get_data(a), grad)\n",
    "\n",
    "\n",
    "class MeanOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a):\n",
    "        return Tensor(np.mean(self.get_data(a)))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        a = self.input[0]\n",
    "        d = self.get_data(a)\n",
    "        return np.full_like(d, grad / d.size)\n",
    "\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data, op=None):\n",
    "        self.data = data\n",
    "        self.op = op\n",
    "        self.grad = 0\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return AddOP()(other, self)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return AddOP()(self, other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return SubOP()(other, self)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return SubOP()(self, other)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return MulOP()(other, self)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return MulOP()(self, other)\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return DivOP()(other, self)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return DivOP()(self, other)\n",
    "\n",
    "    def __neg__(self):\n",
    "        return MulOP()(self, -1)\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        return MatMulOP()(self, other)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.op is not None:\n",
    "            return f\"tensor({self.data}, grad_fn=<{self.op.name}>)\"\n",
    "        else:\n",
    "            return f\"{self.data}\"\n",
    "\n",
    "    def backward(self, grad=1):\n",
    "        self.grad = self.grad + grad  # 就是1\n",
    "        # 假如是一个表达式的计算结果，那么进入这个表达式的backward_native函数中去\n",
    "        if self.op is not None:\n",
    "            self.op.backward_native(grad)\n",
    "\n",
    "\n",
    "class Placeholder(Tensor):\n",
    "    def __init__(self):\n",
    "        super().__init__(0)\n",
    "\n",
    "\n",
    "def SessionRun(var, feed_dict):\n",
    "    for key in feed_dict:\n",
    "        key.data = feed_dict[key]\n",
    "\n",
    "    return var.op.compute()\n",
    "\n",
    "\n",
    "# 模拟包的形式\n",
    "class morch:\n",
    "\n",
    "    @staticmethod\n",
    "    def exp(value):\n",
    "        return ExpOP()(value)\n",
    "\n",
    "    @staticmethod\n",
    "    def log(value):\n",
    "        return LogOP()(value)\n",
    "\n",
    "    @staticmethod\n",
    "    def sum(value):\n",
    "        return SumOP()(value)\n",
    "\n",
    "    @staticmethod\n",
    "    def mean(value):\n",
    "        return MeanOP()(value)\n",
    "\n",
    "\n",
    "print(\"==========================================基于PyTorch的自动微分-动态图版本：==========================================\")\n",
    "a = torch.tensor(value, dtype=torch.float32, requires_grad=True)\n",
    "b = torch.tensor(mul_value, dtype=torch.float32, requires_grad=True)\n",
    "t = torch.sum(1 / (1 + torch.exp(-a)) @ b)\n",
    "t.backward()\n",
    "\n",
    "print(\"计算结果是：\", t)\n",
    "print(\"a的导数是：\", a.grad.numpy())\n",
    "print(\"b的导数是：\", b.grad.numpy())\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "print(\"==========================================自己写的自动微分-静态图版本：==========================================\")\n",
    "# 当你执行完表达式时，就等同于构建了一个计算图。通过计算图反推即可得到梯度\n",
    "a = Placeholder()   #  shape     None, H, W,  C\n",
    "b = Tensor(mul_value)   #  可训练的参数，比如初始化为 高斯初始化，凯明初始化，常量初始化\n",
    "t = morch.sum(1 / (1 + morch.exp(-a)) @ b)\n",
    "\n",
    "# 构建计算图\n",
    "\n",
    "out = SessionRun(t, {a: value})\n",
    "out.backward()\n",
    "\n",
    "print(\"计算结果是：\", out)\n",
    "print(\"dt/da：\", a.grad)\n",
    "print(\"dt/db：\", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
