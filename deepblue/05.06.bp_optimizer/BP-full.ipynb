{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    magic_number, num_samples = struct.unpack(\">ii\", data[:8])\n",
    "    if magic_number != 2049:   # 0x00000801\n",
    "        print(f\"magic number mismatch {magic_number} != 2049\")\n",
    "        return None\n",
    "    \n",
    "    labels = np.frombuffer(data[8:], dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "def load_images(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    magic_number, num_samples, image_width, image_height = struct.unpack(\">iiii\", data[:16])\n",
    "    if magic_number != 2051:   # 0x00000803\n",
    "        print(f\"magic number mismatch {magic_number} != 2051\")\n",
    "        return None\n",
    "    \n",
    "    image_data = np.frombuffer(data[16:], dtype=np.uint8).reshape(num_samples, -1)\n",
    "    return image_data\n",
    "\n",
    "def one_hot(labels, classes, label_smoothing=0):\n",
    "    n = len(labels)\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = load_labels(\"dataset/t10k-labels-idx1-ubyte\")   #  10000,\n",
    "val_images = load_images(\"dataset/t10k-images-idx3-ubyte\")   #  10000, 784\n",
    "val_images = (val_images - np.mean(val_images)) / np.var(val_images)\n",
    "#val_images = val_images / 255 - 0.5\n",
    "\n",
    "train_labels = load_labels(\"dataset/train-labels-idx1-ubyte\") # 60000,\n",
    "train_images = load_images(\"dataset/train-images-idx3-ubyte\") # 60000, 784\n",
    "#train_images = train_images / 255 - 0.5\n",
    "train_images = (train_images - np.mean(train_images)) / np.var(train_images)\n",
    "#train_images = (train_images - np.mean(train_images)) / np.var(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.005390829740138148, 0.03529019335064509)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(val_images), np.max(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        \n",
    "    # 获取他的一个item，  dataset = Dataset(),   dataset[index]\n",
    "    def __getitem__(self, index):\n",
    "        return self.images[index], self.labels[index]\n",
    "    \n",
    "    # 获取数据集的长度，个数\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "class DataLoaderIterator:\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self.cursor = 0\n",
    "        self.indexs = list(range(self.dataloader.count_data))  # 0, ... 60000\n",
    "        if self.dataloader.shuffle:\n",
    "            # 打乱一下\n",
    "            np.random.shuffle(self.indexs)\n",
    "            \n",
    "    def __next__(self):\n",
    "        if self.cursor >= self.dataloader.count_data:\n",
    "            raise StopIteration()\n",
    "            \n",
    "        batch_data = []\n",
    "        remain = min(self.dataloader.batch_size, self.dataloader.count_data - self.cursor)  #  256, 128\n",
    "        for n in range(remain):\n",
    "            index = self.indexs[self.cursor]\n",
    "            data = self.dataloader.dataset[index]\n",
    "            \n",
    "            # 如果batch没有初始化，则初始化n个list成员\n",
    "            if len(batch_data) == 0:\n",
    "                batch_data = [[] for i in range(len(data))]\n",
    "                \n",
    "            #直接append进去\n",
    "            for index, item in enumerate(data):\n",
    "                batch_data[index].append(item)\n",
    "            self.cursor += 1\n",
    "            \n",
    "        # 通过np.vstack一次性实现合并，而非每次一直在合并\n",
    "        for index in range(len(batch_data)):\n",
    "            batch_data[index] = np.vstack(batch_data[index])\n",
    "        return batch_data\n",
    "\n",
    "class DataLoader:\n",
    "    \n",
    "    # shuffle 打乱\n",
    "    def __init__(self, dataset, batch_size, shuffle):\n",
    "        self.dataset = dataset\n",
    "        self.shuffle = shuffle\n",
    "        self.count_data = len(dataset)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return DataLoaderIterator(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.binomial(size=(10, 10), p=0.1, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.train_mode = False\n",
    "        \n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)\n",
    "    \n",
    "    def train(self):\n",
    "        self.train_mode = True\n",
    "        for m in self.modules():\n",
    "            m.train()\n",
    "        \n",
    "    def eval(self):\n",
    "        self.train_mode = False\n",
    "        for m in self.modules():\n",
    "            m.eval()\n",
    "        \n",
    "    def modules(self):\n",
    "        ms = []\n",
    "        for attr in self.__dict__:\n",
    "            m = self.__dict__[attr]\n",
    "            if isinstance(m, Module):\n",
    "                ms.append(m)\n",
    "        return ms\n",
    "    \n",
    "    def params(self):\n",
    "        ps = []\n",
    "        for attr in self.__dict__:\n",
    "            p = self.__dict__[attr]\n",
    "            if isinstance(p, Parameter):\n",
    "                ps.append(p)\n",
    "            \n",
    "        ms = self.modules()\n",
    "        for m in ms:\n",
    "            ps.extend(m.params())\n",
    "        return ps\n",
    "    \n",
    "    def info(self, n):\n",
    "        ms = self.modules()\n",
    "        output = f\"{self.name}\\n\"\n",
    "        for m in ms:\n",
    "            output += ('  '*(n+1)) + f\"{m.info(n+1)}\\n\"\n",
    "        return output[:-1]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.info(0)\n",
    "    \n",
    "class Initializer:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def __call__(self, *args):\n",
    "        return self.apply(*args)\n",
    "        \n",
    "class GaussInitializer(Initializer):\n",
    "    # where :math:`\\mu` is the mean and :math:`\\sigma` the standard\n",
    "    # deviation. The square of the standard deviation, :math:`\\sigma^2`,\n",
    "    # is called the variance.\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def apply(self, value):\n",
    "        value[...] = np.random.normal(self.mu, self.sigma, value.shape)\n",
    "    \n",
    "class Parameter:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.delta = np.zeros(value.shape)\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.delta[...] = 0\n",
    "        \n",
    "class Linear(Module):\n",
    "    def __init__(self, input_feature, output_feature):\n",
    "        super().__init__(\"Linear\")\n",
    "        self.input_feature = input_feature\n",
    "        self.output_feature = output_feature\n",
    "        self.weights = Parameter(np.zeros((input_feature, output_feature)))\n",
    "        self.bias = Parameter(np.zeros((1, output_feature)))\n",
    "        \n",
    "        # 权重初始化 \n",
    "        initer = GaussInitializer(0, np.sqrt(2 / input_feature))  # np.sqrt(2 / input_feature)\n",
    "        initer.apply(self.weights.value)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x_save = x.copy()\n",
    "        return x @ self.weights.value + self.bias.value\n",
    "    \n",
    "    #AB = C  G\n",
    "    #dB = A.T @ G\n",
    "    #dA = G @ B.T\n",
    "    def backward(self, G):\n",
    "        self.weights.delta += self.x_save.T @ G\n",
    "        self.bias.delta += np.sum(G, 0)  #值复制\n",
    "        return G @ self.weights.value.T\n",
    "    \n",
    "class ReLU(Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super().__init__(\"ReLU\")\n",
    "        self.inplace = inplace\n",
    "        \n",
    "    # 亿点点\n",
    "    def forward(self, x):\n",
    "        self.negative_position = x < 0\n",
    "        if not self.inplace:\n",
    "            x = x.copy()\n",
    "            \n",
    "        x[self.negative_position] = 0\n",
    "        return x\n",
    "    \n",
    "    def backward(self, G):\n",
    "        if not self.inplace:\n",
    "            G = G.copy()\n",
    "            \n",
    "        G[self.negative_position] = 0\n",
    "        return G\n",
    "\n",
    "def sigmoid(x):\n",
    "    p0 = x < 0\n",
    "    p1 = ~p0\n",
    "    x = x.copy()\n",
    "\n",
    "    # 如果x的类型是整数，那么会造成丢失精度\n",
    "    x[p0] = np.exp(x[p0]) / (1 + np.exp(x[p0]))\n",
    "    x[p1] = 1 / (1 + np.exp(-x[p1]))\n",
    "    return x\n",
    "\n",
    "class SWish(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"SWish\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x_save = x.copy()\n",
    "        self.sx = sigmoid(x)\n",
    "        return x * self.sx\n",
    "    \n",
    "    def backward(self, G):\n",
    "        return G * (self.sx + self.x_save * self.sx * (1 - self.sx))\n",
    "    \n",
    "class Dropout(Module):\n",
    "    def __init__(self, prob_keep=0.5, inplace=True):\n",
    "        super().__init__(\"Dropout\")\n",
    "        self.prob_keep = prob_keep\n",
    "        self.inplace = inplace\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not self.train_mode:\n",
    "            return x\n",
    "        \n",
    "        self.mask = np.random.binomial(size=x.shape, p=1 - self.prob_keep, n=1)\n",
    "        if not self.inplace:\n",
    "            x = x.copy()\n",
    "            \n",
    "        x[self.mask] = 0\n",
    "        x *= 1 / self.prob_keep\n",
    "        return x\n",
    "    \n",
    "    def backward(self, G):\n",
    "        if not self.inplace:\n",
    "            G = G.copy()\n",
    "        G[self.mask] = 0\n",
    "        G *= 1 / self.prob_keep\n",
    "        return G\n",
    "    \n",
    "class ModuleList(Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(\"ModuleList\")\n",
    "        self.ms = list(args)\n",
    "        \n",
    "    def modules(self):\n",
    "        return self.ms\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for m in self.ms:\n",
    "            x = m(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, G):\n",
    "        for i in range(len(self.ms)-1, -1, -1):\n",
    "            G = self.ms[i].backward(G)\n",
    "        return G\n",
    "    \n",
    "class SigmoidCrossEntropy(Module):\n",
    "    def __init__(self, params, weight_decay=1e-5):\n",
    "        super().__init__(\"CrossEntropyLoss\")\n",
    "        self.params = params\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        #return 1 / (1 + np.exp(-x))\n",
    "        p0 = x < 0\n",
    "        p1 = ~p0\n",
    "        x = x.copy()\n",
    "        x[p0] = np.exp(x[p0]) / (1 + np.exp(x[p0]))\n",
    "        x[p1] = 1 / (1 + np.exp(-x[p1]))\n",
    "        return x\n",
    "    \n",
    "    def decay_loss(self):\n",
    "        loss = 0\n",
    "        for p in self.params:\n",
    "            loss += np.sqrt(np.sum(p.value ** 2)) / (2 * p.value.size) * self.weight_decay\n",
    "        return loss\n",
    "    \n",
    "    def decay_backward(self):\n",
    "        for p in self.params:\n",
    "            eps = 1e-8\n",
    "            p.delta += 1 / (2 * np.sqrt(np.sum(p.value ** 2)) + eps) / (2 * p.value.size) * self.weight_decay * 2 * p.value\n",
    "\n",
    "    def forward(self, x, label_onehot):\n",
    "        eps = 1e-6\n",
    "        self.label_onehot = label_onehot\n",
    "        self.predict = self.sigmoid(x)\n",
    "        self.predict = np.clip(self.predict, a_max=1-eps, a_min=eps)  # 裁切\n",
    "        self.batch_size = self.predict.shape[0]\n",
    "        return -np.sum(label_onehot * np.log(self.predict) + (1 - label_onehot) * \n",
    "                        np.log(1 - self.predict)) / self.batch_size + self.decay_loss()\n",
    "    \n",
    "    def backward(self):\n",
    "        self.decay_backward()\n",
    "        return (self.predict - self.label_onehot) / self.batch_size\n",
    "    \n",
    "class SoftmaxCrossEntropy(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"SoftmaxCrossEntropy\")\n",
    "        \n",
    "    def softmax(self, x):\n",
    "        #return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "        max_x = np.max(x, axis=1, keepdims=True)\n",
    "        exp_x = np.exp(x - max_x)\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, x, label_onehot):\n",
    "        eps = 1e-6\n",
    "        self.label_onehot = label_onehot\n",
    "        self.predict = self.softmax(x)\n",
    "        self.predict = np.clip(self.predict, a_max=1-eps, a_min=eps)  # 裁切\n",
    "        self.batch_size = self.predict.shape[0]\n",
    "        return -np.sum(label_onehot * np.log(self.predict)) / self.batch_size\n",
    "    \n",
    "    def backward(self):\n",
    "        return (self.predict - self.label_onehot) / self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ones((256, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(x, axis=1, keepdims=True).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p(x) = \\frac{1}{\\sqrt{ 2 \\pi \\sigma^2 }}\n",
    "                     e^{ - \\frac{ (x - \\mu)^2 } {2 \\sigma^2} } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, name, model, lr):\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.params = model.params()\n",
    "                \n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.zero_grad()\n",
    "            \n",
    "    def set_lr(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        super().__init__(\"SGD\", model, lr)\n",
    "    \n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.value -= self.lr * param.delta\n",
    "            \n",
    "class SGDMomentum(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3, momentum=0.9):\n",
    "        super().__init__(\"SGDMomentum\", model, lr)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        for param in self.params:\n",
    "            param.v = 0\n",
    "    \n",
    "    # 移动平均\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.v = self.momentum * param.v - self.lr * param.delta\n",
    "            param.value += param.v\n",
    "            \n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3, beta1=0.9, beta2=0.999, l2_regularization = 0):\n",
    "        super().__init__(\"Adam\", model, lr)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.l2_regularization = l2_regularization\n",
    "        self.t = 0\n",
    "        \n",
    "        for param in self.params:\n",
    "            param.m = 0\n",
    "            param.v = 0\n",
    "            \n",
    "    # 指数移动平均\n",
    "    def step(self):\n",
    "        eps = 1e-8\n",
    "        self.t += 1\n",
    "        for param in self.params:\n",
    "            g = param.delta\n",
    "            param.m = self.beta1 * param.m + (1 - self.beta1) * g\n",
    "            param.v = self.beta2 * param.v + (1 - self.beta2) * g ** 2\n",
    "            mt_ = param.m / (1 - self.beta1 ** self.t)\n",
    "            vt_ = param.v / (1 - self.beta2 ** self.t)\n",
    "            param.value -= self.lr * mt_ / (np.sqrt(vt_) + eps) + self.l2_regularization * param.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Module):\n",
    "    def __init__(self, num_feature, num_hidden, num_classes):\n",
    "        super().__init__(\"Model\")\n",
    "        self.backbone = ModuleList(\n",
    "            Linear(num_feature, num_hidden),\n",
    "            ReLU(),\n",
    "            Dropout(),\n",
    "            Linear(num_hidden, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    def backward(self, G):\n",
    "        return self.backbone.backward(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "def estimate_val(predict, gt_labels, classes, loss_func):\n",
    "    plabel = predict.argmax(1)\n",
    "    positive = plabel == gt_labels\n",
    "    total_images = predict.shape[0]\n",
    "    accuracy = sum(positive) / total_images\n",
    "    return accuracy, loss_func(predict, one_hot(gt_labels, classes))\n",
    "\n",
    "def lr_schedule_cosine(lr_min, lr_max, per_epochs):\n",
    "    def compute(epoch):\n",
    "        return lr_min + 0.5 * (lr_max - lr_min) * (1 + np.cos(epoch / per_epochs * np.pi))\n",
    "    return compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 938, 0 / 20, Loss 0.751, LR 0.01\n",
      "Val set, Accuracy: 0.968500, Loss: 0.377\n",
      "Iter 1876, 1 / 20, Loss 0.899, LR 0.01\n",
      "Val set, Accuracy: 0.975500, Loss: 0.280\n",
      "Iter 2814, 2 / 20, Loss 0.221, LR 0.01\n",
      "Val set, Accuracy: 0.978100, Loss: 0.217\n",
      "Iter 3752, 3 / 20, Loss 0.304, LR 0.01\n",
      "Val set, Accuracy: 0.974100, Loss: 0.275\n",
      "Iter 4690, 4 / 20, Loss 0.211, LR 0.01\n",
      "Val set, Accuracy: 0.979300, Loss: 0.187\n",
      "Iter 5628, 5 / 20, Loss 0.213, LR 0.001\n",
      "Val set, Accuracy: 0.985800, Loss: 0.142\n",
      "Iter 6566, 6 / 20, Loss 0.239, LR 0.001\n",
      "Val set, Accuracy: 0.985600, Loss: 0.140\n",
      "Iter 7504, 7 / 20, Loss 0.207, LR 0.001\n",
      "Val set, Accuracy: 0.986200, Loss: 0.135\n",
      "Iter 8442, 8 / 20, Loss 0.203, LR 0.001\n",
      "Val set, Accuracy: 0.986900, Loss: 0.132\n",
      "Iter 9380, 9 / 20, Loss 0.205, LR 0.001\n",
      "Val set, Accuracy: 0.986100, Loss: 0.131\n",
      "Iter 10318, 10 / 20, Loss 0.230, LR 0.001\n",
      "Val set, Accuracy: 0.986600, Loss: 0.128\n",
      "Iter 11256, 11 / 20, Loss 0.205, LR 0.001\n",
      "Val set, Accuracy: 0.986700, Loss: 0.127\n",
      "Iter 12194, 12 / 20, Loss 0.208, LR 0.001\n",
      "Val set, Accuracy: 0.986800, Loss: 0.125\n",
      "Iter 13132, 13 / 20, Loss 0.205, LR 0.001\n",
      "Val set, Accuracy: 0.987300, Loss: 0.122\n",
      "Iter 14070, 14 / 20, Loss 0.207, LR 0.001\n",
      "Val set, Accuracy: 0.987200, Loss: 0.121\n",
      "Iter 15008, 15 / 20, Loss 0.203, LR 0.0001\n",
      "Val set, Accuracy: 0.987400, Loss: 0.121\n",
      "Iter 15946, 16 / 20, Loss 0.201, LR 0.0001\n",
      "Val set, Accuracy: 0.987400, Loss: 0.120\n",
      "Iter 16884, 17 / 20, Loss 0.203, LR 0.0001\n",
      "Val set, Accuracy: 0.987400, Loss: 0.120\n",
      "Iter 17822, 18 / 20, Loss 0.213, LR 1e-05\n",
      "Val set, Accuracy: 0.987300, Loss: 0.120\n",
      "Iter 18760, 19 / 20, Loss 0.208, LR 1e-05\n",
      "Val set, Accuracy: 0.987400, Loss: 0.120\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "classes = 10                  # 定义10个类别\n",
    "batch_size = 64              # 定义每个批次的大小\n",
    "epochs = 20                   # 退出策略，也就是最大把所有数据看10次\n",
    "lr = 1e-2\n",
    "numdata, data_dims = train_images.shape  # 60000, 784\n",
    "\n",
    "# 定义dataloader和dataset，用于数据抓取\n",
    "train_data = DataLoader(Dataset(train_images, one_hot(train_labels, classes)), batch_size, shuffle=True)\n",
    "model = Model(data_dims, 1024, classes)\n",
    "#loss_func = SoftmaxCrossEntropy()\n",
    "loss_func = SigmoidCrossEntropy(model.params(), 0)\n",
    "optim = Adam(model, lr)\n",
    "iters = 0   # 定义迭代次数，因为我们需要展示loss曲线，那么x将会是iters\n",
    "\n",
    "lr_schedule = {\n",
    "    5: 1e-3,\n",
    "    15: 1e-4,\n",
    "    18: 1e-5\n",
    "}\n",
    "\n",
    "# 开始进行epoch循环，总数是epochs次\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    if epoch in lr_schedule:\n",
    "        lr = lr_schedule[epoch]\n",
    "        optim.set_lr(lr)\n",
    "    \n",
    "    model.train()\n",
    "    # 对一个批次内的数据进行迭代，每一次迭代都是一个batch（即256）\n",
    "    for index, (images, labels) in enumerate(train_data):\n",
    "        \n",
    "        x = model(images)\n",
    "        \n",
    "        # 计算loss值\n",
    "        loss = loss_func(x, labels)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        G = loss_func.backward()\n",
    "        model.backward(G)\n",
    "        optim.step()   # 应用梯度，更新参数\n",
    "        iters += 1\n",
    "        \n",
    "    print(f\"Iter {iters}, {epoch} / {epochs}, Loss {loss:.3f}, LR {lr:g}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_accuracy, val_loss = estimate_val(model(val_images), val_labels, classes, loss_func)\n",
    "    print(f\"Val set, Accuracy: {val_accuracy:.6f}, Loss: {val_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 今日总结\n",
    "1. Makefile的使用\n",
    "    - \\\\$\\@ 生成项  \\\\$\\< 依赖项第一个  \\\\$^ 依赖项的所有\n",
    "    - var := \\\\$(shell command)  执行command，结果赋值给var\n",
    "    - 生成项 : 依赖项1 依赖项2 依赖项n\n",
    "         - command\n",
    "    - \\\\$(patsubst src,dst,list)\n",
    "    - 数据类型，字符串，数组，数组以空格区分\n",
    "    - find . -name \"*.cpp\" 查找当前目录下的所有cpp文件\n",
    "    - %.o : %.cpp  通配\n",
    "2. 优化了程序结构\n",
    "    - 使用Layer的方式抽象每一个层\n",
    "    - 使用Model的方式抽象模型\n",
    "    - 使用Parameter抽象可训练参数\n",
    "    - 使用Optimizer抽象优化器\n",
    "    - 引入参数初始化器，高斯初始化\n",
    "    - 加入zero_grad，清空梯度，其实是摆设。多次迭代，一次更新时有用（etc. GAN、强化学习）\n",
    "3. 引入优化器\n",
    "    - Momentum SGD，动量SGD，移动平均，物理解释（算是合理的）是惯性，这个也很常用\n",
    "    - Adam，指数移动平均，这个很常用\n",
    "4. ReLU激活函数\n",
    "    - 在0点的导数，是不可导。可以指定为0或者1，自己去研究\n",
    "5. 没有了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作业\n",
    "1. 周五24：00交\n",
    "2. 交什么？\n",
    "    - a. 实现老师今天总结的BP引入的内容（把这个程序打一遍）\n",
    "    - b. 要求精度大于0.965以上\n",
    "    - c. 精度第一的，考虑奖励一些东西"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
