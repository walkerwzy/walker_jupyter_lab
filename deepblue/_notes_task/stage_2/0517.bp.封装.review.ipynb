{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62e55eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6fdf961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_labels(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        binary_data = f.read()\n",
    "        _, num_items = struct.unpack_from('>II', binary_data, 0)\n",
    "        labels       = struct.unpack_from('B'*num_items, binary_data, 8)\n",
    "        return np.array(labels).reshape(-1, 1).astype(np.int)\n",
    "\n",
    "def decode_images(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        binary_data = f.read()\n",
    "        _,num_images, rows, cols = struct.unpack_from('>IIII', binary_data, 0)\n",
    "        images                   = struct.unpack_from('B'*(num_images*rows*cols), binary_data, 16)\n",
    "        return np.array(images).reshape(-1, rows*cols)\n",
    "\n",
    "filepath = [ \"../stage_1/data/mnist/train-images-idx3-ubyte\",\n",
    "             \"../stage_1/data/mnist/train-labels-idx1-ubyte\",\n",
    "             \"../stage_1/data/mnist/t10k-images-idx3-ubyte\",\n",
    "             \"../stage_1/data/mnist/t10k-labels-idx1-ubyte\"]\n",
    "\n",
    "train_images = decode_images(filepath[0])\n",
    "train_labels = decode_labels(filepath[1])\n",
    "test_images = decode_images(filepath[2])\n",
    "test_labels = decode_labels(filepath[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4d626f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError()\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, image_file, label_file):\n",
    "        self.num_classes = 10\n",
    "        self.images = decode_images(image_file)\n",
    "        self.labels = decode_labels(label_file)\n",
    "        self.images = (self.images / 255.0 - 0.5).astype(np.float32)\n",
    "        # pytorch里面， CELoss使用的不是one_hot\n",
    "        self.labels_one_hot = np.eye(self.num_classes)[self.labels,:]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.images[index], self.labels[index], self.labels_one_hot[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return DataLoader(self)\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return DataLoaderIterator(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        返回一共可以传回多少轮（以每轮batch_size大小为依据）\n",
    "        这里用drop last方案\n",
    "        '''\n",
    "        return len(self.dataset) // self.batch_size\n",
    "    \n",
    "class DataLoaderIterator:\n",
    "    '''\n",
    "    负责一轮数据的打乱，封装\n",
    "    '''\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self.dataset = dataloader.dataset\n",
    "        self.cursor = 0\n",
    "        self.indexs = list(range(len(self.dataset)))\n",
    "        if self.dataloader.shuffle:\n",
    "            np.random.shuffle(self.indexs)\n",
    "        \n",
    "    def __next__(self):\n",
    "        drop_last = len(self.dataset) % self.dataloader.batch_size # drop掉的不需要\n",
    "        if self.cursor >= len(self.dataset) - drop_last:\n",
    "            raise StopIteration()\n",
    "        batch_data = []\n",
    "        for i in range(self.dataloader.batch_size):\n",
    "            index = self.indexs[self.cursor]\n",
    "            item = self.dataloader.dataset[index]\n",
    "            batch_data.append(item)\n",
    "            self.cursor += 1\n",
    "        output = list(zip(*batch_data))  # 此时变成了3组，一每组是(n,)\n",
    "        for i in range(len(output)):\n",
    "            output[i] = np.vstack(output[i]) # 此时把每组里的(n,)变成了(n,1)\n",
    "        return output\n",
    "\n",
    "class Parameter:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.grad = np.zeros_like(data)\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.grad[...] = 0\n",
    "\n",
    "class Module:\n",
    "    '''需求：\n",
    "    backwark, forward\n",
    "    training status\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.training = True\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, g):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)\n",
    "    \n",
    "    def modules(self):\n",
    "        ms = []\n",
    "        for attr in self.__dict__:\n",
    "            m = self.__dict__[attr]\n",
    "            if isinstance(m, Module):\n",
    "                ms.append(m)\n",
    "        return ms\n",
    "    \n",
    "    def params(self):\n",
    "        ps = []\n",
    "        for attr in self.__dict__:\n",
    "            p = self.__dict__[attr]\n",
    "            if isinstance(p, Parameter):\n",
    "                ps.append(p)\n",
    "                \n",
    "        for m in self.modules():\n",
    "            ps.extend(m.params())\n",
    "        \n",
    "        return ps\n",
    "    \n",
    "    def train(self):\n",
    "        self.training = True\n",
    "        for m in self.modules():\n",
    "            m.train()\n",
    "        return self\n",
    "    \n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "        for m in self.modules():\n",
    "            m.eval()\n",
    "        return self\n",
    "    \n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, num_input, num_output):\n",
    "        super().__init__()\n",
    "        # parameters with kaiming init\n",
    "        self.weight = Parameter(np.random.normal(0, 1/np.sqrt(num_input), size=(num_input, num_output)))\n",
    "        self.bias = Parameter(np.zeros((1, num_output)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x @ self.weight.data + self.bias.data\n",
    "    \n",
    "    def backward(self, g):\n",
    "        # update\n",
    "        dw = self.x.T @ g\n",
    "        db = np.sum(g, 0, keepdims=True)\n",
    "        self.weight.grad += dw\n",
    "        self.bias.grad += db\n",
    "        # pass back\n",
    "        dx = g @ self.weight.data.T\n",
    "        return dx\n",
    "        \n",
    "        \n",
    "class Sigmoid(Module):\n",
    "    def sigmoid_fn(self,x):  # sigmoid 函数\n",
    "        xtemp = x.copy()\n",
    "        epx = 0.0001\n",
    "        p = xtemp < 0\n",
    "        p1 = xtemp >= 0\n",
    "        xtemp[p] = np.exp(xtemp[p]) / (np.exp(xtemp[p]) + 1)\n",
    "        xtemp[p1] = 1 / (1 + np.exp(-xtemp[p1]))\n",
    "        return np.clip(xtemp, a_min=epx, a_max=1 - epx)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.y = self.sigmoid_fn(x)\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, g):\n",
    "        return g * self.y * (1-self.y)\n",
    "\n",
    "class Sequencial(Module):\n",
    "    def __init__(self, *items):\n",
    "        super().__init__()\n",
    "        self.items = items\n",
    "        \n",
    "    def modules(self):\n",
    "        return self.items\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for m in self.items:\n",
    "            x = m(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, g):\n",
    "        for m in self.items[::-1]:\n",
    "            g = m.backward(g)\n",
    "        return g\n",
    "    \n",
    "class SoftmaxCrossEntropyLoss(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        '''\n",
    "        f(x) = e^x / sum(e^x)\n",
    "        '''\n",
    "        self.y = y\n",
    "        ex = np.exp(x)\n",
    "        sumex = np.sum(ex, 1, keepdims=True)\n",
    "        self.m = x.shape[0]\n",
    "        self.p = ex / sumex\n",
    "        J = -np.sum(self.y * np.log(self.p)) / self.m\n",
    "        return J\n",
    "        \n",
    "    def backward(self, g):\n",
    "        return g * (self.p - self.y) / self.m\n",
    "    \n",
    "class Network(Module):\n",
    "    def __init__(self, num_feature, num_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        self.layers = Sequencial(\n",
    "            Linear(num_feature, num_hidden),\n",
    "            Sigmoid(),\n",
    "            Linear(num_hidden, num_classes)\n",
    "        )\n",
    "        self.loss = SoftmaxCrossEntropyLoss()\n",
    "        \n",
    "    def inference(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        return self.loss(self.inference(x), target)\n",
    "\n",
    "    def backward(self, g=1):\n",
    "        g = self.loss.backward(g)\n",
    "        return self.layers.backward(g)\n",
    "        \n",
    "class Optimizer:\n",
    "    def __init__(self, params, lr):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.zero_grad()\n",
    "            \n",
    "    def set_lr(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        super().__init__(params, lr)\n",
    "        \n",
    "    def step(self):\n",
    "        for p in self.params:\n",
    "            p.data -= self.lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "580f8281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset = MNISTDataset(\"../stage_1/data/mnist/t10k-images-idx3-ubyte\",\n",
    "#              \"../stage_1/data/mnist/t10k-labels-idx1-ubyte\")\n",
    "# loader = DataLoader(dataset, 256)\n",
    "# for i, (a,b,c) in enumerate(loader):\n",
    "#     print(i, a.shape, b.shape, c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "265ea7f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image, label, one_hot = dataset[5]\n",
    "# image.shape, label, one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9594a32c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.imshow(image.reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2229719a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_hidden = 128\n",
    "num_feature = 784\n",
    "num_classes = 10\n",
    "num_epoch = 10\n",
    "\n",
    "train_data = MNISTDataset(\"../stage_1/data/mnist/train-images-idx3-ubyte\",\n",
    "             \"../stage_1/data/mnist/train-labels-idx1-ubyte\")\n",
    "train_loader = DataLoader(train_data, batch_size, True)\n",
    "test_data = MNISTDataset(\"../stage_1/data/mnist/t10k-images-idx3-ubyte\",\n",
    "             \"../stage_1/data/mnist/t10k-labels-idx1-ubyte\")\n",
    "test_loader = DataLoader(test_data, 512, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ee0865f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0.43/10, loss: 0.538022 accuracy: 80.43%\n",
      "epoch: 0.85/10, loss: 0.442779 accuracy: 86.44%\n",
      "epoch: 1.28/10, loss: 0.336532 accuracy: 88.32%\n",
      "epoch: 1.71/10, loss: 0.392755 accuracy: 88.50%\n",
      "epoch: 2.14/10, loss: 0.221910 accuracy: 89.16%\n",
      "epoch: 2.56/10, loss: 0.293324 accuracy: 90.05%\n",
      "epoch: 2.99/10, loss: 0.278882 accuracy: 90.56%\n",
      "epoch: 3.42/10, loss: 0.307788 accuracy: 89.66%\n",
      "epoch: 3.85/10, loss: 0.206971 accuracy: 90.89%\n",
      "epoch: 4.27/10, loss: 0.233365 accuracy: 90.69%\n",
      "epoch: 4.70/10, loss: 0.171882 accuracy: 91.49%\n",
      "epoch: 5.13/10, loss: 0.208088 accuracy: 91.91%\n",
      "epoch: 5.56/10, loss: 0.162880 accuracy: 92.13%\n",
      "epoch: 5.98/10, loss: 0.178720 accuracy: 92.32%\n",
      "epoch: 6.41/10, loss: 0.216926 accuracy: 92.29%\n",
      "epoch: 6.84/10, loss: 0.178971 accuracy: 92.43%\n",
      "epoch: 7.26/10, loss: 0.194361 accuracy: 92.69%\n",
      "epoch: 7.69/10, loss: 0.183625 accuracy: 92.83%\n",
      "epoch: 8.12/10, loss: 0.123009 accuracy: 92.74%\n",
      "epoch: 8.55/10, loss: 0.110284 accuracy: 92.96%\n",
      "epoch: 8.97/10, loss: 0.117449 accuracy: 93.20%\n",
      "epoch: 9.40/10, loss: 0.154708 accuracy: 93.34%\n",
      "epoch: 9.83/10, loss: 0.247322 accuracy: 93.41%\n"
     ]
    }
   ],
   "source": [
    "network = Network(num_feature, num_hidden, num_classes)\n",
    "optim = SGD(network.params(), 0.8)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for rounds, (imgs, _, one_hots) in  enumerate(train_loader):\n",
    "        niter = epoch * len(train_loader) + rounds  # 每训练完batch_size张图片算1轮，每一个epoch有len(train_load)轮\n",
    "        loss = network(imgs, one_hots)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        network.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        if niter % 100 == 0 and niter > 0:\n",
    "            progress = epoch + rounds / len(train_loader)\n",
    "            correct = 0\n",
    "            for imgs, lbls, _ in test_loader:\n",
    "                probility = network.inference(imgs)\n",
    "                labels = np.argmax(probility, axis=1)\n",
    "                correct += np.sum(labels == lbls[:,0]) \n",
    "            accuracy = correct / len(test_data)\n",
    "            print(f'epoch: {progress:.2f}/{num_epoch}, loss: {loss:.6f} accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccfbd04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
