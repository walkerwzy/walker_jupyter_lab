{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "silver-genius",
   "metadata": {},
   "source": [
    "#### parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "manufactured-wrist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]], requires_grad=True),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " True,\n",
       " None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "param = nn.Parameter(torch.zeros(3, 3))\n",
    "\n",
    "param, param.data, param.requires_grad, param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-graduate",
   "metadata": {
    "tags": []
   },
   "source": [
    "一个`nn.Sequential`会自动把上一个Module的输出当作下一个Module的输入，就像我们自己写的那样   \n",
    "否则就自己去接：\n",
    "```python\n",
    "def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return self.conv2(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-lexington",
   "metadata": {},
   "source": [
    "### 初始化\n",
    "\n",
    "线性函数会自己用凯明[初始化](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py#L81)\n",
    "\n",
    "如果要自己初始化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "measured-listening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2936,  1.0011,  0.4703],\n",
       "        [ 0.2000, -0.1920, -0.8351],\n",
       "        [-1.8805, -0.0365,  0.8782]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.data.normal_()  # 大部分pytorch函数加_尾缀都是为了执行_inplace操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-recruitment",
   "metadata": {},
   "source": [
    "如果对nn的Module里的参数初始化：\n",
    "\n",
    "```python\n",
    "conv1 = torch.nn.Conv2d(...)\n",
    "torch.nn.init.xavier_uniform(conv1.weight)\n",
    "```\n",
    "\n",
    "或者手动调整：\n",
    "```python\n",
    "conv1.weight.data.fill_(0.01)\n",
    "conv1.bias.data.fill_(0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hearing-trailer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2936,  1.0011,  0.4703],\n",
       "        [ 0.2000, -0.1920, -0.8351],\n",
       "        [-1.8805, -0.0365,  0.8782]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.data.normal_()  # 大部分pytorch函数加_尾缀都是为了执行_inplace操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-virgin",
   "metadata": {},
   "source": [
    "### 通过torch.save(obj, f)，保存state_dict为文件\n",
    "\n",
    "- 注意存的是stae_dict()\n",
    "- 注意cpu()，下次加载就会在cpu，如果要改变加载位置，用`map_location`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cleared-wales",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight',\n",
       "              tensor([[[[ 0.1695, -0.1040,  0.3645],\n",
       "                        [-0.3489, -0.0117,  0.1016]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2587,  0.0651,  0.3172],\n",
       "                        [-0.3226,  0.2848,  0.3283]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0018, -0.3298, -0.3942],\n",
       "                        [-0.1472, -0.2767, -0.1312]]]])),\n",
       "             ('conv1.bias', tensor([ 0.2073, -0.1674,  0.3022])),\n",
       "             ('conv2.weight',\n",
       "              tensor([[[[ 0.0219, -0.0274, -0.0278],\n",
       "                        [ 0.1360,  0.1432,  0.0516],\n",
       "                        [-0.0298,  0.1339,  0.1325]],\n",
       "              \n",
       "                       [[-0.0822,  0.1861, -0.1133],\n",
       "                        [ 0.1399, -0.0471,  0.0366],\n",
       "                        [-0.0168,  0.1499,  0.1609]],\n",
       "              \n",
       "                       [[ 0.0304, -0.0666, -0.1714],\n",
       "                        [-0.0512,  0.0941,  0.1615],\n",
       "                        [-0.1528,  0.0840, -0.1441]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1714,  0.1324,  0.1787],\n",
       "                        [-0.0794,  0.0132, -0.0309],\n",
       "                        [-0.0516, -0.0360,  0.1079]],\n",
       "              \n",
       "                       [[-0.1025,  0.0868, -0.1534],\n",
       "                        [-0.0272, -0.1201, -0.1661],\n",
       "                        [ 0.1068, -0.1795, -0.0244]],\n",
       "              \n",
       "                       [[ 0.1600,  0.1703, -0.0669],\n",
       "                        [-0.1867,  0.1083, -0.0030],\n",
       "                        [-0.0336,  0.1526,  0.1366]]]])),\n",
       "             ('conv2.bias', tensor([0.1355, 0.1485]))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, (2, 3))\n",
    "        self.conv2 = nn.Conv2d(3, 2, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return self.conv2(x)\n",
    "\n",
    "model = Model()\n",
    "model.cpu()\n",
    "torch.save(model.state_dict(), \"data/model.pth\")\n",
    "loaded_model = torch.load(\"data/model.pth\") \n",
    "# loaded_model = torch.load(\"data/model.pth\", map_location='cuda:3') \n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-amplifier",
   "metadata": {},
   "source": [
    "### 最大池化，\n",
    "\n",
    "```python\n",
    "torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "```\n",
    "\n",
    "* ceil_mode指定对于输出的计算是否向上取整\n",
    "* 输出的大小：\n",
    "- Input: $ (N, C, H_{in}, W_{in}) $\n",
    "- Output: $ (N, C, H_{out}, W_{out}) $\n",
    "    $$\n",
    "      H_{out} = \\left\\lfloor\\frac{H_{in} + 2 * \\text{padding[0]} - \\text{dilation[0]}\n",
    "            \\times (\\text{kernel_size[0]} - 1) - 1}{\\text{stride[0]}} + 1\\right\\rfloor\n",
    "    $$\n",
    "\n",
    "    $$\n",
    "      W_{out} = \\left\\lfloor\\frac{W_{in} + 2 * \\text{padding[1]} - \\text{dilation[1]}\n",
    "            \\times (\\text{kernel_size[1]} - 1) - 1}{\\text{stride[1]}} + 1\\right\\rfloor\n",
    "    $$\n",
    "- 形象就用小窗口在图片上移动，根据步长等信息来计算pool之后的大小\n",
    "- 2D的pool至少要有3维。（为什么）？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "interpreted-edition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 9, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(2, 1, 28, 28)\n",
    "m = torch.nn.MaxPool2d(3, stride=3)\n",
    "m(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-facility",
   "metadata": {},
   "source": [
    "### 交叉熵损失：\n",
    "\n",
    "```python\n",
    "torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n",
    "```\n",
    "\n",
    "* 多元交叉熵公式：$ Loss = -sum(y * \\log(p))$ 通常配合Softmax实现多分类\n",
    "* 二元交叉熵：$ Loss = -sum(y * \\log(p) + (1 - y) * \\log(1 - p)) $  通常配合Sigmoid实现二分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "involved-scientist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0043, 0.1056, 0.2858],\n",
       "         [0.0270, 0.4716, 0.0601]]),\n",
       " tensor(1.1889))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch   = 2\n",
    "classes = 3\n",
    "torch.manual_seed(3)\n",
    "x = torch.rand(batch, classes)   # predict  2x3\n",
    "y = torch.tensor([1, 2])         # classes\n",
    "entloss = torch.nn.CrossEntropyLoss()\n",
    "x, entloss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "coral-bread",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot\n",
    "one_hot = torch.nn.functional.one_hot(y, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "temporal-family",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1889)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -(one_hot.float() * torch.log(torch.softmax(x, dim=1))).sum() / batch # 应该按行softmax才对啊？\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bulgarian-southwest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-politics",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
