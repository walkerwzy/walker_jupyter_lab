{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OP:\n",
    "    def __init__(self):\n",
    "        self.name = self.__class__.__name__\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        self.input = args\n",
    "        self.output = self.forward(*args)\n",
    "        self.output.op = self\n",
    "        return self.output\n",
    "\n",
    "    def forward(self, *args):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, *args):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_native(self, grad):\n",
    "\n",
    "        input_grads = self.backward(grad)\n",
    "        if not isinstance(input_grads, tuple):\n",
    "            input_grads = (input_grads, )\n",
    "\n",
    "        assert len(input_grads) == len(self.input), \"Number grads mismatch number input\"\n",
    "\n",
    "        for ig, ip in zip(input_grads, self.input):\n",
    "            if isinstance(ip, Tensor):\n",
    "                ip.backward(ig)\n",
    "\n",
    "    def get_data(self, item):\n",
    "        if isinstance(item, Tensor):\n",
    "            return item.data\n",
    "        else:\n",
    "            return item\n",
    "\n",
    "\n",
    "class AddOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return Tensor(self.get_data(a) + self.get_data(b))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad, grad\n",
    "\n",
    "\n",
    "class SubOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return Tensor(self.get_data(a) - self.get_data(b))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad, -1 * grad\n",
    "\n",
    "\n",
    "class MulOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return Tensor(self.get_data(a) * self.get_data(b))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        a, b = self.input\n",
    "        return grad * self.get_data(b), grad * self.get_data(a)\n",
    "\n",
    "\n",
    "class DivOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return Tensor(self.get_data(a) / self.get_data(b))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        a, b = self.input\n",
    "        return grad / self.get_data(b), grad * self.get_data(a) / (self.get_data(b) ** 2) * -1\n",
    "\n",
    "\n",
    "class ExpOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a):\n",
    "        return Tensor(np.exp(self.get_data(a)))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad * np.exp(self.get_data(self.input[0]))\n",
    "\n",
    "\n",
    "class LogOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a):\n",
    "        return Tensor(np.log(self.get_data(a)))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad / self.get_data(self.input[0])\n",
    "\n",
    "\n",
    "class MatMulOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        return Tensor(self.get_data(a) @ self.get_data(b))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        a, b = self.input\n",
    "        return grad @ self.get_data(b).T, self.get_data(a).T @ grad\n",
    "\n",
    "\n",
    "class SumOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a):\n",
    "        return Tensor(np.sum(self.get_data(a)))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        a = self.input[0]\n",
    "        return np.full_like(self.get_data(a), grad)\n",
    "\n",
    "\n",
    "class MeanOP(OP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a):\n",
    "        return Tensor(np.mean(self.get_data(a)))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        a = self.input[0]\n",
    "        d = self.get_data(a)\n",
    "        return np.full_like(d, grad / d.size)\n",
    "\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data, op=None):\n",
    "        self.data = np.array(data, dtype=np.float32)\n",
    "        self.op = op\n",
    "        self.grad = 0\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return AddOP()(other, self)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return AddOP()(self, other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return SubOP()(other, self)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return SubOP()(self, other)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return MulOP()(other, self)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return MulOP()(self, other)\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return DivOP()(other, self)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return DivOP()(self, other)\n",
    "\n",
    "    def __neg__(self):\n",
    "        return MulOP()(self, -1)\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        return MatMulOP()(self, other)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.op is not None:\n",
    "            return f\"tensor({self.data}, grad_fn=<{self.op.name}>)\"\n",
    "        else:\n",
    "            return f\"{self.data}\"\n",
    "\n",
    "    def backward(self, grad=1):\n",
    "        self.grad = self.grad + grad\n",
    "        if self.op is not None:\n",
    "            self.op.backward_native(grad)\n",
    "\n",
    "\n",
    "# 模拟包的形式\n",
    "class morch:\n",
    "\n",
    "    @staticmethod\n",
    "    def exp(value):\n",
    "        return ExpOP()(value)\n",
    "\n",
    "    @staticmethod\n",
    "    def log(value):\n",
    "        return LogOP()(value)\n",
    "\n",
    "    @staticmethod\n",
    "    def sum(value):\n",
    "        return SumOP()(value)\n",
    "\n",
    "    @staticmethod\n",
    "    def mean(value):\n",
    "        return MeanOP()(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于PyTorch的自动微分-动态图版本：==========================================\n",
      "计算结果是： tensor(12.53067, grad_fn=<SumBackward0>)\n",
      "a的导数是： [[0.09375 0.29492 0.27561]\n",
      " [0.01694 0.02649 0.01745]\n",
      " [0.00092 0.00137 0.00088]]\n",
      "b的导数是： [[2.4501  2.4501  2.4501 ]\n",
      " [2.71216 2.71216 2.71216]\n",
      " [2.87377 2.87377 2.87377]]\n",
      "\n",
      "\n",
      "\n",
      "自己写的自动微分-动态图版本：==========================================\n",
      "tensor([[-0. -1. -2.]\n",
      " [-3. -4. -5.]\n",
      " [-6. -7. -8.]], grad_fn=<MulOP>)\n",
      "计算结果是： tensor(12.530673027038574, grad_fn=<SumOP>)\n",
      "a的导数是： [[0.09375 0.29492 0.27561]\n",
      " [0.01694 0.02649 0.01745]\n",
      " [0.00092 0.00137 0.00088]]\n",
      "b的导数是： [[2.4501  2.4501  2.4501 ]\n",
      " [2.71216 2.71216 2.71216]\n",
      " [2.87377 2.87377 2.87377]]\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(precision=5)\n",
    "np.set_printoptions(precision=5)\n",
    "\n",
    "\n",
    "value = np.arange(9).reshape(3, 3).astype(np.float32)\n",
    "mul_value = np.linspace(0, 1, 9).reshape(3, 3)\n",
    "\n",
    "\n",
    "\n",
    "print(\"基于PyTorch的自动微分-动态图版本：==========================================\")\n",
    "a = torch.tensor(value, dtype=torch.float32, requires_grad=True)\n",
    "b = torch.tensor(mul_value, dtype=torch.float32, requires_grad=True)\n",
    "t = torch.sum(1 / (1 + torch.exp(-a)) @ b)\n",
    "t.backward()\n",
    "\n",
    "print(\"计算结果是：\", t)\n",
    "print(\"a的导数是：\", a.grad.numpy())\n",
    "print(\"b的导数是：\", b.grad.numpy())\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"自己写的自动微分-动态图版本：==========================================\")\n",
    "# 当你执行完表达式时，就等同于构建了一个计算图。通过计算图反推即可得到梯度\n",
    "a = Tensor(value)\n",
    "b = Tensor(mul_value)\n",
    "t = morch.sum(1 / (1 + morch.exp(-a)) @ b)\n",
    "t.backward()\n",
    "\n",
    "print(\"计算结果是：\", t)\n",
    "print(\"a的导数是：\", a.grad)\n",
    "print(\"b的导数是：\", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.5",
   "language": "python",
   "name": "torch1.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
