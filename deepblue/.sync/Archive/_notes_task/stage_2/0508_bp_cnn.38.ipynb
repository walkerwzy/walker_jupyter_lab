{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "preliminary-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "upset-combat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_labels(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        binary_data = f.read()\n",
    "    _, num_items = struct.unpack_from('>II', binary_data, 0)\n",
    "    labels       = struct.unpack_from('B'*num_items, binary_data, 8)\n",
    "    return np.array(labels).reshape(-1, 1).astype(np.int)\n",
    "\n",
    "def decode_images(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        binary_data = f.read()\n",
    "    _,num_images, rows, cols = struct.unpack_from('>IIII', binary_data, 0)\n",
    "    images                   = struct.unpack_from('B'*(num_images*rows*cols), binary_data, 16)\n",
    "    return np.array(images).reshape(-1, rows*cols)\n",
    "\n",
    "def one_hot(y, num_classes):\n",
    "    rows   = y.shape[0]\n",
    "    output = np.zeros((rows, num_classes), np.uint8)\n",
    "    for i in range(rows):\n",
    "        output[i, y[i]] = 1 # y的值是几，就把1更新到第几列，行号取决于第几个出现\n",
    "    return output\n",
    "\n",
    "def norm_image(image):  # 将图像进行标准化\n",
    "    return (image / 255 - 0.5).astype(np.float32)\n",
    "\n",
    "y = np.array([1,3,6,7,2,5,8])[...,None]\n",
    "one_hot(y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "impressed-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self, image_file, label_file, num_classes=10):\n",
    "        self.images      = decode_images(image_file)\n",
    "        self.labels      = decode_labels(label_file)\n",
    "        self.onehot      = one_hot(self.labels, num_classes)\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''重载索引器'''\n",
    "        return self.images[index], self.labels[index], self.onehot[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        self.dataset    = dataset\n",
    "        self.shuffle    = shuffle\n",
    "        self.count      = len(dataset)\n",
    "        self.batch_size =  batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return DataLoaderIterator(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''batch count'''\n",
    "        return math.ceil(len(self.dataset) / self.batch_size)\n",
    "    \n",
    "class DataLoaderIterator:\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self.cursor     = 0\n",
    "        self.indexs     = list(range(self.dataloader.count))\n",
    "        if self.dataloader.shuffle:\n",
    "            np.random.shuffle(self.indexs)\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self.cursor >= self.dataloader.count:\n",
    "            raise StopIteration()\n",
    "        \n",
    "        output    = []\n",
    "        one_batch = min(self.dataloader.batch_size, self.dataloader.count - self.cursor)\n",
    "        for _ in range(one_batch):\n",
    "            index = self.indexs[self.cursor]\n",
    "            data  = self.dataloader.dataset[index]\n",
    "            output.append(data)\n",
    "            self.cursor += 1\n",
    "#         output = np.split(output, [1,2], 1)# list(range(1, len(data)-1)), 1)\n",
    "        output = list(zip(*output))\n",
    "        for i in range(len(output)):\n",
    "            output[i] = np.vstack(output[i])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "genuine-gravity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(loader) =  40\n",
      "0 (256, 784) (256, 1) (256, 10)\n",
      "1 (256, 784) (256, 1) (256, 10)\n",
      "2 (256, 784) (256, 1) (256, 10)\n",
      "3 (256, 784) (256, 1) (256, 10)\n",
      "4 (256, 784) (256, 1) (256, 10)\n",
      "5 (256, 784) (256, 1) (256, 10)\n",
      "6 (256, 784) (256, 1) (256, 10)\n",
      "7 (256, 784) (256, 1) (256, 10)\n",
      "8 (256, 784) (256, 1) (256, 10)\n",
      "9 (256, 784) (256, 1) (256, 10)\n",
      "10 (256, 784) (256, 1) (256, 10)\n",
      "11 (256, 784) (256, 1) (256, 10)\n",
      "12 (256, 784) (256, 1) (256, 10)\n",
      "13 (256, 784) (256, 1) (256, 10)\n",
      "14 (256, 784) (256, 1) (256, 10)\n",
      "15 (256, 784) (256, 1) (256, 10)\n",
      "16 (256, 784) (256, 1) (256, 10)\n",
      "17 (256, 784) (256, 1) (256, 10)\n",
      "18 (256, 784) (256, 1) (256, 10)\n",
      "19 (256, 784) (256, 1) (256, 10)\n",
      "20 (256, 784) (256, 1) (256, 10)\n",
      "21 (256, 784) (256, 1) (256, 10)\n",
      "22 (256, 784) (256, 1) (256, 10)\n",
      "23 (256, 784) (256, 1) (256, 10)\n",
      "24 (256, 784) (256, 1) (256, 10)\n",
      "25 (256, 784) (256, 1) (256, 10)\n",
      "26 (256, 784) (256, 1) (256, 10)\n",
      "27 (256, 784) (256, 1) (256, 10)\n",
      "28 (256, 784) (256, 1) (256, 10)\n",
      "29 (256, 784) (256, 1) (256, 10)\n",
      "30 (256, 784) (256, 1) (256, 10)\n",
      "31 (256, 784) (256, 1) (256, 10)\n",
      "32 (256, 784) (256, 1) (256, 10)\n",
      "33 (256, 784) (256, 1) (256, 10)\n",
      "34 (256, 784) (256, 1) (256, 10)\n",
      "35 (256, 784) (256, 1) (256, 10)\n",
      "36 (256, 784) (256, 1) (256, 10)\n",
      "37 (256, 784) (256, 1) (256, 10)\n",
      "38 (256, 784) (256, 1) (256, 10)\n",
      "39 (16, 784) (16, 1) (16, 10)\n"
     ]
    }
   ],
   "source": [
    "# 测试DataLoader\n",
    "dataset = DataSet(\"../stage_1/data/mnist/t10k-images-idx3-ubyte\", \"../stage_1/data/mnist/t10k-labels-idx1-ubyte\")\n",
    "loader = DataLoader(dataset, 256)\n",
    "print('len(loader) = ', len(loader))\n",
    "for ibatch, (images, labels, targets) in enumerate(loader):\n",
    "    print(ibatch, images.shape, labels.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44d5f6a5-d189-45da-a1c9-935c696366bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemm_conv(tensor, kernel):\n",
    "    print(tensor.shape, kernel.shape)\n",
    "    _, kh, kw = kernel.shape\n",
    "    n, c, h, w = tensor.shape\n",
    "    s = (w - kw + 1) * (h - kh + 1)\n",
    "    column = np.zeros((n, kh * kw * c, s))\n",
    "    col_kernel = kernel.reshape(1, -1)\n",
    "    half_kx = kw // 2\n",
    "    half_ky = kh // 2\n",
    "    ksize = kw * kh\n",
    "    \n",
    "    for m in range(n):\n",
    "        for ic in range(c):\n",
    "            col_x = 0\n",
    "            for iy in range(half_ky, h - half_ky):\n",
    "                for ix in range(half_kx, w - half_kx):\n",
    "                    for iky in range(kh):\n",
    "                        for ikx in range(kw):\n",
    "                            pixel_value = tensor[m, ic, iy - half_ky + iky, ix - half_kx + ikx]\n",
    "                            col_y = ic * ksize + ikx + iky * kw\n",
    "                            column[m, col_y, col_x] = pixel_value\n",
    "                    col_x += 1\n",
    "    output = col_kernel @ column\n",
    "#     output = output.reshape(n, h - kh + 1, -1)\n",
    "    return ourput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ahead-triangle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        self.train_mode = True\n",
    "        \n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)\n",
    "    \n",
    "    def train(self):\n",
    "        self.train_mode = True\n",
    "        for m in self.modules():\n",
    "            m.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        self.train_mode = False\n",
    "        for m in self.modules():\n",
    "            m.eval()\n",
    "    \n",
    "    def modules(self):\n",
    "        ms = []\n",
    "        # 反射\n",
    "        for attr in self.__dict__:\n",
    "            m = self.__dict__[attr]\n",
    "            if isinstance(m, Module):\n",
    "                ms.append(m)\n",
    "        return ms\n",
    "    \n",
    "    def params(self):\n",
    "        ps = []\n",
    "        for attr in self.__dict__:\n",
    "            p = self.__dict__[attr]\n",
    "            if isinstance(p, Parameter):\n",
    "                ps.append(p)\n",
    "            \n",
    "        ms = self.modules()\n",
    "        for m in ms:\n",
    "            ps.extend(m.params())\n",
    "        return ps\n",
    "    \n",
    "    def info(self, n):\n",
    "        ms = self.modules()\n",
    "        name = self.__class__.__name__\n",
    "        output = f\"{name}\\n\"\n",
    "        for m in ms:\n",
    "            output += ('  '*(n+1)) + f\"{m.info(n+1)}\\n\"\n",
    "        return output[:-1]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.info(0)\n",
    "    \n",
    "class Parameter:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.grad = np.zeros(data.shape)\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.grad[...] = 0\n",
    "        \n",
    "class Linear(Module):\n",
    "    def __init__(self, in_number, out_number):\n",
    "        super().__init__()\n",
    "        self.weight   = Parameter(np.random.normal(0, np.sqrt(2/(in_number+out_number)), size=(in_number, out_number)))\n",
    "        self.bias     = Parameter(np.zeros(out_number))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x @ self.weight.data + self.bias.data\n",
    "    \n",
    "    def backward(self, g):\n",
    "        '''\n",
    "        g是上层的gradient\n",
    "        这里有x, weight, bias三个变量，所以要分别提供对三者的求导\n",
    "        其中weight, bias是参数，求出来后直接消费掉\n",
    "        对x变量的求导要传给下一层\n",
    "        '''\n",
    "        # 对g补齐对weight的链式求导\n",
    "        self.weight.grad += self.x.T @ g\n",
    "        # 对g补齐对bias的链式求导\n",
    "        self.bias.grad   += np.sum(g, axis=0)\n",
    "        # 对下层返对x的导数\n",
    "        return g @ self.weight.data.T\n",
    "    \n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def sigmoid_impl(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return self.sigmoid_impl(x)\n",
    "    \n",
    "    def backward(self, g):\n",
    "        return g * self.sigmoid_impl(self.x) * (1 - self.sigmoid_impl(self.x))\n",
    "    \n",
    "class Dropout(Module):\n",
    "    '''\n",
    "    通过对输入的参数值乘以采用指定概率的伯努利分布随机数，\n",
    "    实现随机将部分值设置为0，同时其导数也会为0，称之为失活\n",
    "    '''\n",
    "    def __init__(self, prob_keep=0.5, inplace=True):\n",
    "        super().__init__()\n",
    "        self.prob_keep = prob_keep\n",
    "        self.inplace   = inplace\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.train_mode:\n",
    "            return x\n",
    "        if not self.inplace:\n",
    "            x          = x.copy()\n",
    "        self.mask      = np.random.binomial(1, 1-self.prob_keep, x.shape)\n",
    "        x[self.mask]   = 0         # fancing boolean index\n",
    "        x *= 1 / self.prob_keep    # 一部分x置零了，剩下的部分就把值扩大相应倍数\n",
    "        return x\n",
    "        \n",
    "    def backward(self, g):\n",
    "        if not self.inplace:\n",
    "            g = g.copy()\n",
    "        g[self.mask] = 0\n",
    "        g *= 1 / self.prob_keep\n",
    "        return g\n",
    "\n",
    "class Conv2d(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 先把数据摆成[图片，通道，行，列]的格式\n",
    "        w = h  = int(np.sqrt(len(x[0])))\n",
    "        tensor = np.array([])\n",
    "        for row in x:\n",
    "            # 每一行摆回一张图片，\n",
    "            # 单通道图片，把最里层转为数组，以复用多通道代码\n",
    "            image   = row.reshape((w, h))[..., None]\n",
    "            image   = image.transpose(2, 0, 1)[None] # 再外包一层，以便于竖向拼接\n",
    "            if len(tensor) == 0:\n",
    "                tensor = image\n",
    "            else:\n",
    "                tensor = np.vstack([tensor, image])\n",
    "        # 28*28 -> 28*28*1 -> (1, 1, 28, 28)\n",
    "        self.kernel = np.random.randn(3, 3)[None]\n",
    "        x = gemm_conv(tensor, self.kernel)   # img2col  <- col_kernel @ col_image\n",
    "        return x\n",
    "    \n",
    "    def backward(self, g):\n",
    "        col_kernel = self.kernel.reshape(1, -1)\n",
    "        return col_kernel.T @ g\n",
    "    \n",
    "class Sequencial(Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.items = list(args)\n",
    "        \n",
    "    def modules(self):\n",
    "        return self.items\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for m in self.items:\n",
    "            x = m(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, G):\n",
    "        for item in self.items[::-1]:\n",
    "            G = item.backward(G)\n",
    "        return G\n",
    "    \n",
    "# 包含了softmax操作和loss计算，返回的是loss\n",
    "class SoftmaxCrossEntropyLoss(Module):  # 定义损失函数\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def softmax(self, predict):  # 定义softmax的计算方法\n",
    "        exp_predict = np.exp(predict)\n",
    "        total = np.sum(exp_predict, axis=1, keepdims=True)\n",
    "        return exp_predict / total\n",
    "    \n",
    "    def forward(self, x, y):  # 前向传播\n",
    "        '''\n",
    "        比二元交叉熵少半截，即(1-y) * log(1-y)的部分\n",
    "        '''\n",
    "        self.batch_size  = len(x)\n",
    "        self.probability = self.softmax(x)\n",
    "        self.y           = y\n",
    "        return -np.sum(y * np.log(self.probability)) / self.batch_size\n",
    "    \n",
    "    def backward(self, g=1):  # 反向传播\n",
    "        '''自己推推试试'''\n",
    "        g = g * self.probability\n",
    "        return (g - self.y) / self.batch_size\n",
    "    \n",
    "class Network(Module):\n",
    "    def __init__(self, num_feature, num_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        self.layers = Sequencial(\n",
    "            Conv2d(),\n",
    "            Linear(num_feature, num_hidden),\n",
    "            Sigmoid(),\n",
    "            Dropout(0.75),\n",
    "            Linear(num_hidden, num_classes)\n",
    "        )\n",
    "        self.lossfn = SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        return self.lossfn(self(x), y)\n",
    "    \n",
    "    def backward(self):\n",
    "        g = self.lossfn.backward()\n",
    "        self.layers.backward(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ideal-agreement",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, params, lr):\n",
    "        self.lr = lr\n",
    "        self.params = params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.zero_grad()\n",
    "            \n",
    "    def set_lr(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "        \n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        super().__init__(params, lr)\n",
    "    \n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.data -= self.lr * param.grad\n",
    "            \n",
    "            \n",
    "class SGDMomentum(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, momentum=0.9):\n",
    "        super().__init__(params, lr)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        for param in self.params:\n",
    "            param.v = 0\n",
    "    \n",
    "    # 移动平均\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.v = self.momentum * param.v - self.lr * param.grad\n",
    "            param.data += param.v\n",
    "            \n",
    "            \n",
    "class AdamW(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, beta1=0.9, beta2=0.999, l2_regularization = 0):\n",
    "        super().__init__(params, lr)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.l2_regularization = l2_regularization\n",
    "        self.t = 0\n",
    "        \n",
    "        for param in self.params:\n",
    "            param.m = 0\n",
    "            param.v = 0\n",
    "            \n",
    "    # 指数移动平均\n",
    "    def step(self):\n",
    "        eps = 1e-8\n",
    "        self.t += 1\n",
    "        for param in self.params:\n",
    "            g = param.grad + self.l2_regularization * param.data\n",
    "            param.m = self.beta1 * param.m + (1 - self.beta1) * g\n",
    "            param.v = self.beta2 * param.v + (1 - self.beta2) * g ** 2\n",
    "            mt_ = param.m / (1 - self.beta1 ** self.t)\n",
    "            vt_ = param.v / (1 - self.beta2 ** self.t)\n",
    "            param.data -= self.lr * mt_ / (np.sqrt(vt_) + eps)\n",
    "            \n",
    "            \n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, beta1=0.9, beta2=0.999, l2_regularization = 0):\n",
    "        super().__init__(params, lr)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.l2_regularization = l2_regularization\n",
    "        self.t = 0\n",
    "        \n",
    "        for param in self.params:\n",
    "            param.m = 0\n",
    "            param.v = 0\n",
    "            \n",
    "    # 指数移动平均\n",
    "    def step(self):\n",
    "        eps = 1e-8\n",
    "        self.t += 1\n",
    "        for param in self.params:\n",
    "            g = param.grad\n",
    "            param.m = self.beta1 * param.m + (1 - self.beta1) * g\n",
    "            param.v = self.beta2 * param.v + (1 - self.beta2) * g ** 2\n",
    "            mt_ = param.m / (1 - self.beta1 ** self.t)\n",
    "            vt_ = param.v / (1 - self.beta2 ** self.t)\n",
    "            param.data -= self.lr * mt_ / (np.sqrt(vt_) + eps) + self.l2_regularization * param.data\n",
    "\n",
    "def save_model(file, model):\n",
    "    \n",
    "    # 改成只保存parameter\n",
    "    with open(file, \"wb\") as f:\n",
    "        f.write(pickle.dumps(model))\n",
    "        \n",
    "def load_model(file):\n",
    "    \n",
    "    # 改成只保存parameter\n",
    "    with open(file, \"rb\") as f:\n",
    "        return pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "annual-acceptance",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "batch_size  = 256\n",
    "num_hidden  = 256   # 做一个隐层，取256个特征值\n",
    "num_classes = 10    # 分类的数量\n",
    "# num_feature = 784   \n",
    "num_feature = 676   # 3x3 kernel (28-2)^2\n",
    "\n",
    "train_dataset = DataSet(\"../stage_1/data/mnist/train-images-idx3-ubyte\", \"../stage_1/data/mnist/train-labels-idx1-ubyte\", num_classes)\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_dataset = DataSet(\"../stage_1/data/mnist/t10k-images-idx3-ubyte\", \"../stage_1/data/mnist/t10k-labels-idx1-ubyte\", num_classes)\n",
    "test_loader = DataLoader(test_dataset, 1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "destroyed-convention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network\n",
      "  Sequencial\n",
      "    Conv2d\n",
      "    Linear\n",
      "    Sigmoid\n",
      "    Dropout\n",
      "    Linear\n",
      "  SoftmaxCrossEntropyLoss\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(1, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(2, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(3, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(4, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(5, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(6, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(7, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(8, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(9, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(10, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(11, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(12, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(13, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(14, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(15, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(16, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(17, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(18, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(19, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(20, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(21, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(22, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(23, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(24, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(25, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(26, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(27, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(28, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(29, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(30, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(31, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(32, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(33, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(34, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(35, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(36, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(37, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(38, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(39, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(40, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(41, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(42, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(43, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(44, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(45, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(46, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(47, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(48, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(49, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(50, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(51, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(52, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(53, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(54, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(55, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(56, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(57, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(58, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(59, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(60, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(61, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(62, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(63, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(64, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(65, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(66, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(67, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(68, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(69, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(70, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(71, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(72, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(73, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(74, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(75, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(76, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(77, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(78, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(79, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(80, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(81, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(82, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(83, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(84, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(85, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(86, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(87, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(88, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(89, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(90, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(91, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(92, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(93, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(94, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(95, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(96, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(97, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(98, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(99, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(100, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(101, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(102, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(103, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(104, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(105, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(106, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(107, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(108, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(109, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(110, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(111, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(112, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(113, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(114, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(115, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(116, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(117, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(118, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(119, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(120, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(121, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(122, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(123, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(124, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(125, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(126, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(127, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(128, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(129, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(130, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(131, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(132, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(133, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(134, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(135, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(136, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(137, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(138, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(139, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(140, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(141, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(142, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(143, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(144, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(145, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(146, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(147, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(148, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(149, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(150, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(151, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(152, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(153, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(154, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(155, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(156, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(157, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(158, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(159, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(160, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(161, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(162, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(163, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(164, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(165, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(166, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(167, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(168, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(169, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(170, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(171, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(172, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(173, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(174, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(175, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(176, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(177, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(178, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(179, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(180, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(181, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(182, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(183, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(184, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(185, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(186, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(187, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(188, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(189, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(190, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(191, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(192, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(193, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(194, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(195, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(196, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(197, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(198, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(199, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(200, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(201, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(202, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(203, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(204, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(205, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(206, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(207, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(208, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(209, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(210, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(211, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(212, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(213, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(214, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(215, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(216, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(217, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(218, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(219, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(220, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(221, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(222, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(223, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(224, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(225, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(226, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(227, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(228, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(229, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(230, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(231, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(232, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(233, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(234, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(235, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(236, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(237, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(238, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(239, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(240, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(241, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(242, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(243, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(244, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(245, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(246, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(247, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(248, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(249, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(250, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(251, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(252, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(253, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(254, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(255, 1, 28, 28)\n",
      "(28, 28, 1)\n",
      "(1, 1, 28, 28)\n",
      "(256, 1, 28, 28)\n",
      "(256, 1, 28, 28) (1, 3, 3)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ourput' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-0eb36d7d7f51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mniter\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mepoch_index\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtrain_round\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mround_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mn_imgs\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mnorm_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_imgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mloss\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_onehots\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-91-07ced4a77d2f>\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlossfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-91-07ced4a77d2f>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-91-07ced4a77d2f>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-91-07ced4a77d2f>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-91-07ced4a77d2f>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-91-07ced4a77d2f>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-91-07ced4a77d2f>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;31m# (28, 28, 1, 1, 256)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgemm_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# img2col  <- col_kernel @ col_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-75074b13d431>\u001b[0m in \u001b[0;36mgemm_conv\u001b[1;34m(tensor, kernel)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol_kernel\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m#     output = output.reshape(n, h - kh + 1, -1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mourput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ourput' is not defined"
     ]
    }
   ],
   "source": [
    "train_round    = len(train_loader)  # 每一次训练要几轮才做完\n",
    "train_epochs   = 10                 # 共做几次训练\n",
    "lr_schedule    = {0:1e-3, 8:1e-2}   # 学习率策略\n",
    "network        = Network(num_feature, num_hidden, num_classes)\n",
    "optim          = AdamW(network.params(), 1e-3, l2_regularization=1e-4)\n",
    "\n",
    "print(network)\n",
    "for epoch_index in range(train_epochs):\n",
    "    if epoch_index in lr_schedule:\n",
    "        optim.set_lr(lr_schedule[epoch_index])\n",
    "    \n",
    "    # dataloader已经实现了按batch_size输出\n",
    "    for round_index, (t_imgs, t_lbls, t_onehots) in enumerate(train_loader):\n",
    "        niter   = epoch_index * train_round + round_index\n",
    "        n_imgs  = norm_image(t_imgs)\n",
    "        loss    = network.loss(n_imgs, t_onehots)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        network.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        # 每训练100张图片计算一次精度\n",
    "        if niter % 100 == 0 and niter > 0:\n",
    "            progress    = epoch_index + round_index / train_round\n",
    "            correct     = 0\n",
    "            for ts_imgs, ts_lbls, _ in test_loader:\n",
    "                p       = network(norm_image(ts_imgs))\n",
    "                labels  = np.argmax(p, axis=1)\n",
    "                correct = correct + (labels == ts_lbls[:,0]).sum()\n",
    "            accuracy    = correct / len(test_loader.dataset)\n",
    "            print(f'Iter: {niter:04d}, Epoch: {progress:.2f}/{train_epochs}, LR: {optim.lr:.6f}, Loss: {loss:.6f}, Test Accuracy: {accuracy}')\n",
    "            \n",
    "    save_model('model_0508', network)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-liability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "\n",
    "network = load_model(\"model_0508\")\n",
    "show_rows = 5 # 五行\n",
    "show_cols = 5 # 五列\n",
    "select_count = show_rows * show_cols\n",
    "select_iter = iter(test_loader)\n",
    "test_batch_images, test_batch_labels, test_batch_onehots = next(select_iter)\n",
    "\n",
    "predict        = network(norm_image(test_batch_images))  # 进行网络预测\n",
    "predict_labels = predict.argmax(axis=1)  # 获取预测的标签值\n",
    "\n",
    "plt.figure(figsize=(show_rows * 5, show_cols * 5))  # 定义画板的大小\n",
    "for i in range(select_count):  # 显示图片和标题\n",
    "\n",
    "    plt.subplot(show_rows, show_cols, i+1)\n",
    "\n",
    "    predict_label      = predict_labels[i]\n",
    "    ground_truth_label = test_batch_labels[i, 0]\n",
    "    image              = test_batch_images[i].reshape(28, 28)\n",
    "\n",
    "    plt.title(f\"Predict: {predict_label}, GroundTruth: {ground_truth_label}\")\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-smile",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
