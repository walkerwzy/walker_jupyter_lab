{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vocal-contact",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "structural-durham",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_labels(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        binary_data = f.read()\n",
    "    _, num_items = struct.unpack_from('>II', binary_data, 0)\n",
    "    labels       = struct.unpack_from('B'*num_items, binary_data, 8)\n",
    "    return np.array(labels).reshape(-1, 1).astype(np.int)\n",
    "\n",
    "def decode_images(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        binary_data = f.read()\n",
    "    _,num_images, rows, cols = struct.unpack_from('>IIII', binary_data, 0)\n",
    "    images                   = struct.unpack_from('B'*(num_images*rows*cols), binary_data, 16)\n",
    "    return np.array(images).reshape(-1, rows*cols)\n",
    "\n",
    "def one_hot(y, num_classes):\n",
    "    rows   = y.shape[0]\n",
    "    output = np.zeros((rows, num_classes), np.uint8)\n",
    "    for i in range(rows):\n",
    "        output[i, y[i]] = 1 # y的值是几，就把1更新到第几列，行号取决于第几个出现\n",
    "    return output\n",
    "\n",
    "def norm_image(image):  # 将图像进行标准化\n",
    "    return (image / 255 - 0.5).astype(np.float32)\n",
    "\n",
    "y = np.array([1,3,6,7,2,5,8])[...,None]\n",
    "# one_hot(y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brilliant-belly",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self, image_file, label_file, num_classes=10):\n",
    "        self.images      = decode_images(image_file)\n",
    "        self.labels      = decode_labels(label_file)\n",
    "        self.onehot      = one_hot(self.labels, num_classes)\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''重载索引器'''\n",
    "        return self.images[index], self.labels[index], self.onehot[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        self.dataset    = dataset\n",
    "        self.shuffle    = shuffle\n",
    "        self.count      = len(dataset)\n",
    "        self.batch_size =  batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return DataLoaderIterator(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''batch count'''\n",
    "        return math.ceil(len(self.dataset) / self.batch_size)\n",
    "    \n",
    "class DataLoaderIterator:\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self.cursor     = 0\n",
    "        self.indexs     = list(range(self.dataloader.count))\n",
    "        if self.dataloader.shuffle:\n",
    "            np.random.shuffle(self.indexs)\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self.cursor >= self.dataloader.count:\n",
    "            raise StopIteration()\n",
    "        \n",
    "        output    = []\n",
    "        one_batch = min(self.dataloader.batch_size, self.dataloader.count - self.cursor)\n",
    "        for _ in range(one_batch):\n",
    "            index = self.indexs[self.cursor]\n",
    "            data  = self.dataloader.dataset[index]\n",
    "            output.append(data)\n",
    "            self.cursor += 1\n",
    "#         output = np.split(output, [1,2], 1)# list(range(1, len(data)-1)), 1)\n",
    "        output = list(zip(*output))\n",
    "        for i in range(len(output)):\n",
    "            output[i] = np.vstack(output[i])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ultimate-dayton",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(loader) =  40\n"
     ]
    }
   ],
   "source": [
    "# 测试DataLoader\n",
    "dataset = DataSet(\"../stage_1/data/mnist/t10k-images-idx3-ubyte\", \"../stage_1/data/mnist/t10k-labels-idx1-ubyte\")\n",
    "loader = DataLoader(dataset, 256)\n",
    "print('len(loader) = ', len(loader))\n",
    "# for ibatch, (images, labels, targets) in enumerate(loader):\n",
    "#     print(ibatch, images.shape, labels.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "guilty-hammer",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "batch_size  = 256\n",
    "num_hidden  = 256   # 做一个隐层，取256个特征值\n",
    "num_classes = 10    # 分类的数量\n",
    "# num_feature = 784   \n",
    "num_feature = 676   # 3x3 kernel (28-2)^2\n",
    "\n",
    "train_dataset = DataSet(\"../stage_1/data/mnist/train-images-idx3-ubyte\", \"../stage_1/data/mnist/train-labels-idx1-ubyte\", num_classes)\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_dataset = DataSet(\"../stage_1/data/mnist/t10k-images-idx3-ubyte\", \"../stage_1/data/mnist/t10k-labels-idx1-ubyte\", num_classes)\n",
    "test_loader = DataLoader(test_dataset, 1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beautiful-source",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def gemm_im2col(tensor, kernel):\n",
    "#     e.g. tensor:(1024, 28, 28), kernel:(1, 9)\n",
    "#     print(f'tensor:{tensor.shape}, kernel:{kernel.shape}')\n",
    "#     _, kh, kw = kernel.shape\n",
    "    kh, kw  = 3, 3\n",
    "    c, h, w = tensor.shape\n",
    "    ksize = kw * kh\n",
    "    s = (w - kw + 1) * (h - kh + 1)  # s:单图列数， ksize: 行数， c:图片张数  s*c=总列数\n",
    "#     column = np.zeros((1, kh * kw * c, s))\n",
    "    column = np.zeros((ksize, s * c))\n",
    "    col_kernel = kernel.reshape(1, -1) \n",
    "    half_kx = kw // 2\n",
    "    half_ky = kh // 2\n",
    "\n",
    "    for ic in range(c):\n",
    "        for iy in range(half_ky, h - half_ky):\n",
    "            for ix in range(half_kx, w - half_kx):\n",
    "                for iky in range(kh):\n",
    "                    col_y = 0\n",
    "                    for ikx in range(kw):\n",
    "                        pixel_value = tensor[ic, iy - half_ky + iky, ix - half_kx + ikx]\n",
    "                        col_x = ic * ksize + (ix-half_kx) + (iy-half_ky)\n",
    "                        col_y = ikx + iky * kw\n",
    "                        column[col_y, col_x] = pixel_value\n",
    "#   e.g. output = (1, 9) x (9, 173056)\n",
    "#   print(f'output = {col_kernel.shape} x {column.shape}')\n",
    "    output = col_kernel @ column\n",
    "    return output, column\n",
    "\n",
    "def gemm_col2im_index(row, col, ctr, k_size=3):\n",
    "    '''\n",
    "        row: 行索引（其实就是在kernel形状内移动几次）\n",
    "        col: 列索引\n",
    "        ctr: kernel窗口在图片内移动多少次换行\n",
    "        k_size: kernel核大小，3x3即传3\n",
    "\n",
    "        左上角=(col//ctr, col%ctr)\n",
    "        行偏移：row // ksize\n",
    "        列偏移：row % ksize\n",
    "    \n",
    "    '''\n",
    "    start = (col // ctr, col % ctr)\n",
    "    rowindex = start[0] + row // k_size\n",
    "    colindex = start[1] + row % k_size\n",
    "    return (rowindex, colindex)\n",
    "\n",
    "def gemm_col2im(column, imgcols, ctr, ker_size, outimage):\n",
    "    '''\n",
    "        imgcols: column化的图片的列数，如784, 676\n",
    "        ctr: kernel能在一张图片里横向滑动的次数\n",
    "        ker_size: 比如3\n",
    "    '''\n",
    "    height, width = column.shape\n",
    "    for row in range(height):     # kernel元素个数决定行数\n",
    "        for col in range(width):  # 28*28 -> 26*26 -> 决定列数\n",
    "            batch     = col // imgcols\n",
    "            c         = col % imgcols  # 即每26*26换了一张图，列索引归零\n",
    "            row_index, col_index = gemm_col2im_index(row, c, ctr, ker_size)\n",
    "            outimage[batch, row_index, col_index] += column[row, col]\n",
    "    return outimage\n",
    "\n",
    "def lr_cosine_schedule(lr_min, lr_max, epochs):\n",
    "    '''\n",
    "        :param epochs: total epochs are performed before a new restart.\n",
    "        :param epoch: How many epochs have been performed since the last restart.\n",
    "        :return: a function to compute a value within a period.\n",
    "    '''\n",
    "\n",
    "    def compute(epoch):\n",
    "        return lr_min + 0.5 * (lr_max - lr_min) * (1 + np.cos(epoch / epochs * np.pi))\n",
    "\n",
    "    return compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-reaction",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Xavier初始化公式xavier normal为$(0, std^2)$：$std = gain \\times \\sqrt{\\frac{2}{fan\\_in + fan\\_out}}$\n",
    "    - fan_in为输入的数据量，fan_out为输出的数据量\n",
    "    - Linear中，就是输入通道数和输出通道数\n",
    "    - Conv中，则需要计算通道与宽高的乘积\n",
    "- Xavier考虑的非线性激活函数为：TanH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "collective-portfolio",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        self.train_mode = True\n",
    "        \n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)\n",
    "    \n",
    "    def train(self):\n",
    "        self.train_mode = True\n",
    "        for m in self.modules():\n",
    "            m.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        self.train_mode = False\n",
    "        for m in self.modules():\n",
    "            m.eval()\n",
    "    \n",
    "    def modules(self):\n",
    "        ms = []\n",
    "        # 反射\n",
    "        for attr in self.__dict__:\n",
    "            m = self.__dict__[attr]\n",
    "            if isinstance(m, Module):\n",
    "                ms.append(m)\n",
    "        return ms\n",
    "    \n",
    "    def params(self):\n",
    "        ps = []\n",
    "        for attr in self.__dict__:\n",
    "            p = self.__dict__[attr]\n",
    "            if isinstance(p, Parameter):\n",
    "                ps.append(p)\n",
    "            \n",
    "        ms = self.modules()\n",
    "        for m in ms:\n",
    "            ps.extend(m.params())\n",
    "        return ps\n",
    "    \n",
    "    def info(self, n):\n",
    "        ms = self.modules()\n",
    "        name = self.__class__.__name__\n",
    "        output = f\"{name}\\n\"\n",
    "        for m in ms:\n",
    "            output += ('  '*(n+1)) + f\"{m.info(n+1)}\\n\"\n",
    "        return output[:-1]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.info(0)\n",
    "    \n",
    "class Parameter:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.grad = np.zeros(data.shape)\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.grad[...] = 0\n",
    "        \n",
    "class Linear(Module):\n",
    "    def __init__(self, in_number, out_number):\n",
    "        super().__init__()\n",
    "        self.weight   = Parameter(np.random.normal(0, np.sqrt(2/(in_number+out_number)), size=(in_number, out_number)))\n",
    "        self.bias     = Parameter(np.zeros(out_number))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x @ self.weight.data + self.bias.data\n",
    "    \n",
    "    def backward(self, g):\n",
    "        '''\n",
    "        g是上层的gradient\n",
    "        这里有x, weight, bias三个变量，所以要分别提供对三者的求导\n",
    "        其中weight, bias是参数，求出来后直接消费掉\n",
    "        对x变量的求导要传给下一层\n",
    "        '''\n",
    "        # 对g补齐对weight的链式求导\n",
    "        self.weight.grad += self.x.T @ g\n",
    "        # 对g补齐对bias的链式求导\n",
    "        self.bias.grad   += np.sum(g, axis=0)\n",
    "        # 用修正后的参数继续对下层返对x的导数\n",
    "        return (g @ self.weight.data.T).reshape(g.shape[0], -1)\n",
    "    \n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def sigmoid_impl(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return self.sigmoid_impl(x)\n",
    "    \n",
    "    def backward(self, g):\n",
    "        return g * self.sigmoid_impl(self.x) * (1 - self.sigmoid_impl(self.x))\n",
    "\n",
    "class ReLU(Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_negative = x < 0\n",
    "        if not self.inplace:\n",
    "            x = x.copy()\n",
    "\n",
    "        x[self.x_negative] = 0  # boolean indexing\n",
    "        return x\n",
    "\n",
    "    def backward(self, G):\n",
    "        if not self.inplace:\n",
    "            G = G.copy()\n",
    "\n",
    "        G[self.x_negative] = 0\n",
    "        return G\n",
    "\n",
    "class Dropout(Module):\n",
    "    '''\n",
    "    通过对输入的参数值乘以采用指定概率的伯努利分布随机数，\n",
    "    实现随机将部分值设置为0，同时其导数也会为0，称之为失活\n",
    "    '''\n",
    "    def __init__(self, prob_keep=0.5, inplace=True):\n",
    "        super().__init__()\n",
    "        self.prob_keep = prob_keep\n",
    "        self.inplace   = inplace\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.train_mode:\n",
    "            return x\n",
    "        if not self.inplace:\n",
    "            x          = x.copy()\n",
    "        self.mask      = np.random.binomial(1, 1-self.prob_keep, x.shape)\n",
    "        x[self.mask]   = 0         # fancing boolean index\n",
    "        x *= 1 / self.prob_keep    # 一部分x置零了，剩下的部分就把值扩大相应倍数\n",
    "        return x\n",
    "        \n",
    "    def backward(self, g):\n",
    "        if not self.inplace:\n",
    "            g = g.copy()\n",
    "        g[self.mask] = 0\n",
    "        g *= 1 / self.prob_keep\n",
    "        return g\n",
    "\n",
    "class Conv2d(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.k_size = 3\n",
    "        # 一张图片一个输出就一行参数，用这一行参数训练一组（比如256张）图片\n",
    "        self.weight = Parameter(np.random.randn(1, self.k_size * self.k_size)) # 3x3\n",
    "        self.bias   = Parameter(np.zeros((1, 1)))\n",
    "        \n",
    "        # 直接传图片进来就不费这些事了，先不改\n",
    "        rows, cols = x.shape  # e.g. (256, 764)\n",
    "        w = h  = int(np.sqrt(cols))\n",
    "        # 28*28 -> (1, 28, 28), -> (256, 28, 28)\n",
    "        tensor = np.array([])\n",
    "        for row in x:\n",
    "            image   = row.reshape((w, h))[None] # 再外包一层，以便于竖向拼接\n",
    "            if len(tensor) == 0:\n",
    "                tensor = image\n",
    "            else:\n",
    "                tensor = np.vstack([tensor, image])\n",
    "        kernel   = self.weight.data\n",
    "        conv_img, col_img = gemm_im2col(tensor, kernel)   # img2col  <- col_kernel @ col_image\n",
    "        self.x   = col_img\n",
    "        return (conv_img + self.bias.data).reshape(rows, -1)\n",
    "\n",
    "    def backward(self, g):\n",
    "            '''\n",
    "                tensor:(256, 28, 28), kernel:(1, 9)\n",
    "                output = w @ x = (1, 9) x (9, 173056) .reshape(256, -1) = (256, 676)\n",
    "\n",
    "                d(w @ x)/dw => g @ x.T\n",
    "                g = (256, 676) -> reshape(1, -1) -> (1, 173056)\n",
    "                x = (9, 173056) -> transpose\n",
    "                -> (1, 173056) @ (173056, 9) => (1, 9)\n",
    "\n",
    "                d(w @ x)/dx  => w.T @ g\n",
    "                g = (256, 676) -> （1, 173056）\n",
    "                w = (1, 9) transpose\n",
    "                -> (9, 1) @ (1, 173056) => (9, 17...) => col2im => (256, 28, 28)\n",
    "                -> reshape(256, -1) => (256, 784)\n",
    "            '''\n",
    "            # d(w @ x) / dw\n",
    "            self.weight.grad += g.reshape(1, -1) @ self.x.T\n",
    "            self.bias.grad   += np.sum(g)\n",
    "\n",
    "            # d(w @ x) / dx\n",
    "            column     = self.weight.data.T @ g.reshape(1, -1)\n",
    "            batchs, imgcols = g.shape  # (256, 676)\n",
    "\n",
    "            ctr = 26 # <= 28 - 3 + 1 # image shape - kernel shape + 1\n",
    "            ker = 3  # <= kernel size\n",
    "            g_out = np.zeros((batchs, 28, 28))\n",
    "            g_out = gemm_col2im(column, imgcols, ctr, ker, g_out)\n",
    "            return g_out.reshape(256, -1)\n",
    "    \n",
    "# 包含了softmax操作和loss计算，返回的是loss\n",
    "class SoftmaxCrossEntropyLoss(Module):  # 定义损失函数\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def softmax(self, predict):  # 定义softmax的计算方法\n",
    "        exp_predict = np.exp(predict)\n",
    "        total = np.sum(exp_predict, axis=1, keepdims=True)\n",
    "        return exp_predict / total\n",
    "    \n",
    "    def forward(self, x, y):  # 前向传播\n",
    "        '''\n",
    "        比二元交叉熵少半截，即(1-y) * log(1-y)的部分\n",
    "        '''\n",
    "        self.batch_size  = len(x)\n",
    "        self.probability = self.softmax(x)\n",
    "        self.y           = y\n",
    "        return -np.sum(y * np.log(self.probability)) / self.batch_size\n",
    "    \n",
    "    def backward(self, g=1):  # 反向传播\n",
    "        '''自己推推试试'''\n",
    "        g = g * self.probability\n",
    "        return (g - self.y) / self.batch_size\n",
    "     \n",
    "class Sequencial(Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.items = list(args)\n",
    "        \n",
    "    def modules(self):\n",
    "        return self.items\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for m in self.items:\n",
    "            x = m(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, G):\n",
    "        for item in self.items[::-1]:\n",
    "            G = item.backward(G)\n",
    "        return G\n",
    "    \n",
    "class Network(Module):\n",
    "    def __init__(self, num_feature, num_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        self.layers = Sequencial(\n",
    "            Conv2d(), # 先写死，一个3x3kernel，一个输出256x(26x26)，不padding\n",
    "#             Sigmoid(),\n",
    "            ReLU(),\n",
    "            Linear(num_feature, num_classes)\n",
    "        )\n",
    "        self.lossfn = SoftmaxCrossEntropyLoss()\n",
    "        \n",
    "#     def __init__(self, num_feature, num_hidden, num_classes):\n",
    "#         super().__init__()\n",
    "#         num_feature = 784\n",
    "#         self.layers = Sequencial(\n",
    "#             Linear(num_feature, num_hidden),\n",
    "#             Sigmoid(),\n",
    "#             Dropout(0.75),\n",
    "#             Linear(num_hidden, num_classes)\n",
    "#         )\n",
    "#         self.lossfn = SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        return self.lossfn(self(x), y)\n",
    "    \n",
    "    def backward(self):\n",
    "        g = self.lossfn.backward()\n",
    "        self.layers.backward(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "relevant-evaluation",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, params, lr):\n",
    "        self.lr = lr\n",
    "        self.params = params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.zero_grad()\n",
    "            \n",
    "    def set_lr(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "        \n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        super().__init__(params, lr)\n",
    "    \n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.data -= self.lr * param.grad\n",
    "            \n",
    "            \n",
    "class SGDMomentum(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, momentum=0.9):\n",
    "        super().__init__(params, lr)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        for param in self.params:\n",
    "            param.v = 0\n",
    "    \n",
    "    # 移动平均\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.v = self.momentum * param.v - self.lr * param.grad\n",
    "            param.data += param.v\n",
    "            \n",
    "            \n",
    "class AdamW(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, beta1=0.9, beta2=0.999, l2_regularization = 0):\n",
    "        super().__init__(params, lr)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.l2_regularization = l2_regularization\n",
    "        self.t = 0\n",
    "        \n",
    "        for param in self.params:\n",
    "            param.m = 0\n",
    "            param.v = 0\n",
    "            \n",
    "    # 指数移动平均\n",
    "    def step(self):\n",
    "        eps = 1e-8\n",
    "        self.t += 1\n",
    "        for param in self.params:\n",
    "            g = param.grad + self.l2_regularization * param.data\n",
    "            param.m = self.beta1 * param.m + (1 - self.beta1) * g\n",
    "            param.v = self.beta2 * param.v + (1 - self.beta2) * g ** 2\n",
    "            mt_ = param.m / (1 - self.beta1 ** self.t)\n",
    "            vt_ = param.v / (1 - self.beta2 ** self.t)\n",
    "            param.data -= self.lr * mt_ / (np.sqrt(vt_) + eps)\n",
    "            \n",
    "            \n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, beta1=0.9, beta2=0.999, l2_regularization = 0):\n",
    "        super().__init__(params, lr)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.l2_regularization = l2_regularization\n",
    "        self.t = 0\n",
    "        \n",
    "        for param in self.params:\n",
    "            param.m = 0\n",
    "            param.v = 0\n",
    "            \n",
    "    # 指数移动平均\n",
    "    def step(self):\n",
    "        eps = 1e-8\n",
    "        self.t += 1\n",
    "        for param in self.params:\n",
    "            g = param.grad\n",
    "            param.m = self.beta1 * param.m + (1 - self.beta1) * g\n",
    "            param.v = self.beta2 * param.v + (1 - self.beta2) * g ** 2\n",
    "            mt_ = param.m / (1 - self.beta1 ** self.t)\n",
    "            vt_ = param.v / (1 - self.beta2 ** self.t)\n",
    "            param.data -= self.lr * mt_ / (np.sqrt(vt_) + eps) + self.l2_regularization * param.data\n",
    "\n",
    "def save_model(file, model):\n",
    "    \n",
    "    # 改成只保存parameter\n",
    "    with open(file, \"wb\") as f:\n",
    "        f.write(pickle.dumps(model))\n",
    "        \n",
    "def load_model(file):\n",
    "    \n",
    "    # 改成只保存parameter\n",
    "    with open(file, \"rb\") as f:\n",
    "        return pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-produce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network\n",
      "  Sequencial\n",
      "    Conv2d\n",
      "    ReLU\n",
      "    Linear\n",
      "  SoftmaxCrossEntropyLoss\n",
      "Iter: 0100, Epoch: 0.43/10, LR: 0.010000, Loss: 2.427119, Test Accuracy: 0.1137\n",
      "Iter: 0200, Epoch: 0.85/10, LR: 0.010000, Loss: 2.458883, Test Accuracy: 0.1135\n",
      "Iter: 0300, Epoch: 1.28/10, LR: 0.009758, Loss: 2.414146, Test Accuracy: 0.1139\n",
      "Iter: 0400, Epoch: 1.70/10, LR: 0.009758, Loss: 2.365527, Test Accuracy: 0.1136\n",
      "Iter: 0500, Epoch: 2.13/10, LR: 0.009055, Loss: 2.395529, Test Accuracy: 0.113\n",
      "Iter: 0600, Epoch: 2.55/10, LR: 0.009055, Loss: 2.306278, Test Accuracy: 0.1128\n",
      "Iter: 0700, Epoch: 2.98/10, LR: 0.009055, Loss: 2.428323, Test Accuracy: 0.1135\n",
      "Iter: 0800, Epoch: 3.40/10, LR: 0.007960, Loss: 2.300434, Test Accuracy: 0.113\n",
      "Iter: 0900, Epoch: 3.83/10, LR: 0.007960, Loss: 2.301428, Test Accuracy: 0.1133\n",
      "Iter: 1000, Epoch: 4.26/10, LR: 0.006580, Loss: 2.318766, Test Accuracy: 0.1133\n",
      "Iter: 1100, Epoch: 4.68/10, LR: 0.006580, Loss: 2.332414, Test Accuracy: 0.1134\n",
      "Iter: 1200, Epoch: 5.11/10, LR: 0.005050, Loss: 2.302635, Test Accuracy: 0.1131\n",
      "Iter: 1300, Epoch: 5.53/10, LR: 0.005050, Loss: 2.313449, Test Accuracy: 0.1135\n",
      "Iter: 1400, Epoch: 5.96/10, LR: 0.005050, Loss: 2.306010, Test Accuracy: 0.1032\n",
      "Iter: 1500, Epoch: 6.38/10, LR: 0.003520, Loss: 2.332310, Test Accuracy: 0.1124\n",
      "Iter: 1600, Epoch: 6.81/10, LR: 0.003520, Loss: 2.301419, Test Accuracy: 0.1135\n",
      "Iter: 1700, Epoch: 7.23/10, LR: 0.002140, Loss: 2.299313, Test Accuracy: 0.1135\n",
      "Iter: 1800, Epoch: 7.66/10, LR: 0.002140, Loss: 2.303681, Test Accuracy: 0.1136\n"
     ]
    }
   ],
   "source": [
    "train_round    = len(train_loader)  # 每一次训练要几轮才做完\n",
    "train_epochs   = 10                 # 共做几次训练\n",
    "# lr_schedule    = {0:1e-4, 9:1e-4}   # 学习率策略\n",
    "lr_schedule    = lr_cosine_schedule(1e-4, 1e-2, train_epochs)\n",
    "network        = Network(num_feature, num_hidden, num_classes)\n",
    "optim          = AdamW(network.params(), 1e-3, l2_regularization=1e-4)\n",
    "\n",
    "print(network)\n",
    "for epoch_index in range(train_epochs):\n",
    "    optim.set_lr(lr_schedule(epoch_index))\n",
    "    \n",
    "    # dataloader已经实现了按batch_size输出\n",
    "    for round_index, (t_imgs, t_lbls, t_onehots) in enumerate(train_loader):\n",
    "        niter   = epoch_index * train_round + round_index\n",
    "        n_imgs  = norm_image(t_imgs)\n",
    "        loss    = network.loss(n_imgs, t_onehots)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        network.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        # 每训练100张图片计算一次精度\n",
    "        if niter % 100 == 0 and niter > 0:\n",
    "            progress    = epoch_index + round_index / train_round\n",
    "            correct     = 0\n",
    "            for ts_imgs, ts_lbls, _ in test_loader:\n",
    "                p       = network(norm_image(ts_imgs))\n",
    "                labels  = np.argmax(p, axis=1)\n",
    "                correct = correct + (labels == ts_lbls[:,0]).sum()\n",
    "            accuracy    = correct / len(test_loader.dataset)\n",
    "            print(f'Iter: {niter:04d}, Epoch: {progress:.2f}/{train_epochs}, LR: {optim.lr:.6f}, Loss: {loss:.6f}, Test Accuracy: {accuracy}')\n",
    "            \n",
    "#     save_model('model_0508', network)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-rates",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 预测\n",
    "\n",
    "# network = load_model(\"model_0508\")\n",
    "show_rows = 5 # 五行\n",
    "show_cols = 5 # 五列\n",
    "select_count = show_rows * show_cols\n",
    "select_iter = iter(test_loader)\n",
    "test_batch_images, test_batch_labels, test_batch_onehots = next(select_iter)\n",
    "\n",
    "predict        = network(norm_image(test_batch_images))  # 进行网络预测\n",
    "predict_labels = predict.argmax(axis=1)  # 获取预测的标签值\n",
    "\n",
    "plt.figure(figsize=(show_rows * 5, show_cols * 5))  # 定义画板的大小\n",
    "for i in range(select_count):  # 显示图片和标题\n",
    "\n",
    "    plt.subplot(show_rows, show_cols, i+1)\n",
    "\n",
    "    predict_label      = predict_labels[i]\n",
    "    ground_truth_label = test_batch_labels[i, 0]\n",
    "    image              = test_batch_images[i].reshape(28, 28)\n",
    "\n",
    "    plt.title(f\"Predict: {predict_label}, GroundTruth: {ground_truth_label}\")\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-dance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
