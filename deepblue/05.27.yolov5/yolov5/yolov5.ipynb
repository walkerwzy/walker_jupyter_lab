{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolo（you only look once）\n",
    "<img src=\"https://pjreddie.com/media/image/sayit.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 主页：https://pjreddie.com/darknet/yolo/\n",
    "* 背景：\n",
    "    - 一种单阶的目标检测器算法\n",
    "    - 高效、快速\n",
    "    - GitHub: https://github.com/pjreddie/darknet\n",
    "        - 基于C和CUDA、CUDNN实现的框架，叫做darknet，用这个框架训练的Yolo模型\n",
    "        - 从卷积、优化器、到模型、检测器训练全部都是C语言实现的，可以找到实现的每一个代码\n",
    "    - 版本有Yolo、YoloV2、YoloV3、YoloV4、YoloV5\n",
    "        - 其中Yolo、YoloV2、YoloV3，作者Joseph Redmon，也是Yolo的发起人，Darknet的创作者\n",
    "            - 于2020年约2月，宣布退出计算机视觉：https://huanqiukexue.com/a/qianyan/xinxi__nenyuan/2020/0224/29238.html\n",
    "        - 其中YoloV4出现在2020年4月，作者是AlexeyAB：https://github.com/AlexeyAB/darknet\n",
    "            - 一个Yolo的拥护者，为Yolo提供了大量支持，Windows支持友好等工作\n",
    "            - 受到作者点赞\n",
    "            - 依旧基于C语言的darknet实现\n",
    "            - V4的Pytorch实现：https://github.com/WongKinYiu/ScaledYOLOv4\n",
    "        - 其中YoloV5出现在2020年5月，作者是ultralytics：https://github.com/ultralytics/yolov5\n",
    "            - 一个马赛克增广的提出者\n",
    "            - 基于Pytorch的实现版本\n",
    "            - 官网集成：https://pytorch.org/hub/ultralytics_yolov5/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YoloV3结构图：\n",
    "<img src=\"yolov3.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YoloV4结构图：\n",
    "<img src=\"yolov4.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"scaled-yolov4.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YoloV5结构图：\n",
    "<img src=\"yolov5.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概念1：Apex（Automatic Mixed Precision）\n",
    "* 自动混合精度库，用于降低训练显存占用，提升训练速度\n",
    "    - 如果没有FP16支持的显卡，这个选项没有意义\n",
    "        - FP16是半精度的意思，以前的显卡仅仅支持FP32，对于FP16需要进行FP16到FP32转换，然后使用FP32运算，再转换回FP16。如果支持FP16，则是显卡硬件上支持FP16格式数据直接运算。由于计算精度底，相比FP32效率大大提升\n",
    "        - 同样，由于是FP16，16个bit表示浮点数，造成可以表示的范围和精度有限，数值越大，精度越差。与ReLU6相呼应\n",
    "* GitHub地址：https://github.com/NVIDIA/apex\n",
    "    - 安装方法：\n",
    "        ```bash\n",
    "        git clone https://github.com/NVIDIA/apex\n",
    "        cd apex\n",
    "        pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n",
    "        ```\n",
    "        - 如果安装出错，通常都是cuda环境上存在问题，例如：\n",
    "            - CUDA_HOME=xxx，如果写成了CUDA_HOME=$CUDA_HOME:xxx就是有问题的，造成nvcc无法找到。因为需要进行CPP的编译操作\n",
    "            - 找不到cublas_v2.h，也是由于cuda安装时少了东西造成的。重新安装\n",
    "    - 使用方法：\n",
    "        ```Python\n",
    "        from apex import amp\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n",
    "        \n",
    "        # 对于Loss的计算，需要进行梯度裁剪\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概念2：DP（DataParallel）/DDP（DistributedDataParallel）\n",
    "* 多GPU同时训练时的并行方法\n",
    "* DP已经淘汰，速度慢效果差，使用ParameterServer模式，显存占用多\n",
    "* 欢迎来到DDP，基于Ring-AllReduce实现不同显卡的梯度Reduce操作（平均）\n",
    "    - 使用方法：\n",
    "        ```Python\n",
    "        # 初始化后端\n",
    "        import torch.distributed as dist\n",
    "        dist.init_process_group(backend='nccl', init_method='env://')\n",
    "\n",
    "        # 使用同步BN，不同卡之间传递均值方差\n",
    "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)\n",
    "        \n",
    "        # 使用DDP\n",
    "        from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "        model = DDP(model, device_ids=[rank], output_device=rank)\n",
    "        \n",
    "        # 对于DataLoader的处理，需要使用DistributedSampler：\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 num_workers=nw,\n",
    "                                                 sampler=train_sampler,\n",
    "                                                 pin_memory=True,\n",
    "                                                 collate_fn=LoadImagesAndLabels.collate_fn)\n",
    "        \n",
    "        #训练时，设置epoch：dataloader.sampler.set_epoch(epoch)\n",
    "        #其余更正常训练一样\n",
    "        ```\n",
    "    - 带有DDP程序，启动时，第一种做法是基于python命令行：\n",
    "        - 启动4个显卡执行训练，指定给main.py的参数是--program_args=value。master_port指定为6666，如果同时执行不同训练程序，需要指定不同master_port\n",
    "        - CUDA_VISIBLE_DEVICES=\"0,1,2,3\"是设置环境变量，让当前程序执行时，0，1，2，3显卡可见\n",
    "        ```bash\n",
    "        > CUDA_VISIBLE_DEVICES=\"0,1,2,3\" python -m torch.distributed.launch --nproc_per_node 4 --master_port 6666 main.py --program_args=value\n",
    "        ```\n",
    "        - 对于main.py，需要接受一个参数--local_rank，该local_rank指定为序号，0、1、2、3。通过区分local_rank，实现不同逻辑，例如local_rank=0时打印loss和存储模型等等\n",
    "    - 带有DDP程序，启动时，第二种做法是基于torch.multiprocessing.spawn方法实现\n",
    "        - 该方法实现可以直接启动就是多卡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trick\n",
    "1. tqdm库，可以实现计算时的进度条并且告诉你时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For index = 100: 100%|██████████| 100/100 [00:10<00:00,  9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_sum = 5050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "value_sum = 0\n",
    "miter = range(1, 101)\n",
    "bar = tqdm(miter, desc=\"For循环\")\n",
    "for i in bar:\n",
    "    time.sleep(0.1)\n",
    "    value_sum += i\n",
    "    bar.set_description(f\"For index = {i}\")\n",
    "\n",
    "print(f\"value_sum = {value_sum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. nms，可以使用torchvision.ops.nms函数实现GPU加速版本的nms计算，高效率"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
